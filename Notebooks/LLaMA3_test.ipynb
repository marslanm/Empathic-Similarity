{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bf381cd-0051-4b5f-a758-2f58c5342e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading shards: 100%|█████████████████████████| 4/4 [05:15<00:00, 78.91s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:01<00:00,  3.49it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# import transformers\n",
    "# import torch\n",
    "\n",
    "# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# pipeline = transformers.pipeline(\n",
    "#   \"text-generation\",\n",
    "#   model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "#   model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "#   device=\"cuda\",\n",
    "# )\n",
    "# run above to download and load models\n",
    "# models are saved in cd ~/.cache/huggingface/hub/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cffad04-f339-4728-9e0e-d4334457d12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLaMA3 input format\n",
    "<|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "    {{ system_prompt }}<|eot_id|>\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    {{ user_msg_1 }}<|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    {{ model_answer_1 }}<|eot_id|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1092da72-b4f6-43ee-b2e4-6e1f75de3fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "<|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "    You are a mental health supporter, you'd like to help others.<|eot_id|>\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "        I feel so sad, I lost my dog.<|eot_id|>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cbc300-04ef-409c-808c-caff166b7fbb",
   "metadata": {},
   "source": [
    "### VLLM inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e6719c3-8c9d-4f9a-9476-d103671cd867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-05-01 12:44:39,169\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-05-01 12:44:41,447\tINFO worker.py:1749 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-01 12:44:41 llm_engine.py:98] Initializing an LLM engine (v0.4.1) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-01 12:44:45 utils.py:608] Found nccl from library /home/yuxiawang/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=10165)\u001b[0m INFO 05-01 12:44:45 utils.py:608] Found nccl from library /home/yuxiawang/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-01 12:44:45 selector.py:28] Using FlashAttention backend.\n",
      "\u001b[36m(RayWorkerWrapper pid=10165)\u001b[0m INFO 05-01 12:44:45 selector.py:28] Using FlashAttention backend.\n",
      "INFO 05-01 12:44:45 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=10165)\u001b[0m INFO 05-01 12:44:45 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "INFO 05-01 12:44:45 utils.py:129] reading GPU P2P access cache from /home/yuxiawang/.config/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "WARNING 05-01 12:44:45 custom_all_reduce.py:74] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[36m(RayWorkerWrapper pid=10165)\u001b[0m INFO 05-01 12:44:45 utils.py:129] reading GPU P2P access cache from /home/yuxiawang/.config/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[36m(RayWorkerWrapper pid=10165)\u001b[0m WARNING 05-01 12:44:45 custom_all_reduce.py:74] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 05-01 12:44:46 weight_utils.py:193] Using model weights format ['*.safetensors']\n",
      "\u001b[36m(RayWorkerWrapper pid=10165)\u001b[0m INFO 05-01 12:44:46 weight_utils.py:193] Using model weights format ['*.safetensors']\n",
      "INFO 05-01 12:44:58 model_runner.py:173] Loading model weights took 7.4829 GB\n",
      "\u001b[36m(RayWorkerWrapper pid=10165)\u001b[0m INFO 05-01 12:44:58 model_runner.py:173] Loading model weights took 7.4829 GB\n",
      "INFO 05-01 12:45:00 ray_gpu_executor.py:217] # GPU blocks: 12254, # CPU blocks: 4096\n",
      "INFO 05-01 12:45:00 model_runner.py:976] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-01 12:45:00 model_runner.py:980] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=10165)\u001b[0m INFO 05-01 12:45:00 model_runner.py:976] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerWrapper pid=10165)\u001b[0m INFO 05-01 12:45:00 model_runner.py:980] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\n",
      "\u001b[36m(RayWorkerWrapper pid=10165)\u001b[0m [rank1]:[W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-01 12:45:03 model_runner.py:1057] Graph capturing finished in 3 secs.\n",
      "\u001b[36m(RayWorkerWrapper pid=10165)\u001b[0m INFO 05-01 12:45:03 model_runner.py:1057] Graph capturing finished in 3 secs.\n"
     ]
    }
   ],
   "source": [
    "import vllm\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model = vllm.LLM(\n",
    "    model=model_name,\n",
    "    tokenizer=model_name,\n",
    "    tensor_parallel_size=2,\n",
    "    dtype=\"bfloat16\" if torch.cuda.is_bf16_supported() else \"float16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e83a1fd-f7b1-433b-a654-cac11bc1f64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# print(tokenizer.chat_template)\n",
    "# set generation parameters\n",
    "sampling_params = vllm.SamplingParams(max_tokens=256)\n",
    "print(sampling_params)\n",
    "\n",
    "system_prompt = \"\"\"You are a mental health supporter, you'd like to help others.\"\"\"\n",
    "samples = [\"I feel so sad, I lost my dog.\", \"I was promoted to manager recently, but my colleagues seem not happy.\"]\n",
    "model_inputs = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\":\"system\", \"content\": system_prompt.strip()}, \n",
    "            {\"role\":\"user\", \"content\": sample.strip()}\n",
    "        ], \n",
    "        tokenize=False)\n",
    "    for sample in samples\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1abb3bae-55cd-4c40-aff2-5750fcc364bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a mental health supporter, you'd like to help others.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I feel so sad, I lost my dog.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(model_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faa20352-41f1-47d7-87df-d046108dd0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 2/2 [00:02<00:00,  1.42s/it]\n"
     ]
    }
   ],
   "source": [
    "result = model.generate(model_inputs, sampling_params)\n",
    "responses = [ x.outputs[0].text.replace(\"<|start_header_id|>assistant<|end_header_id|>\", \"\").strip() for x in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e6fda29-6d70-4dc0-923c-8b1babcd1709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm so sorry to hear that you're going through this. Losing a pet is never easy, and it's completely normal to feel overwhelmed with emotions.\\n\\nFirst of all, please know that you're not alone. Many people have been in your shoes, and it's okay to grieve the loss of your furry friend. Would you like to talk about your dog and the memories you shared with them? Sometimes sharing stories and stories can help process our emotions and feel a bit better.\\n\\nIt's also important to acknowledge that grief is a journey, and it's okay to take your time. You might need to take things one day at a time, and that's okay. You don't have to rush the process.\\n\\nAdditionally, have you considered talking to a mental health professional? They can offer you a safe and non-judgmental space to process your emotions and provide you with coping strategies to deal with your grief. Would you like me to help you find some resources or information on how to get started with that?\\n\\nRemember, you're doing the best you can, and that's something to be proud of.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54afd9f2-729d-456e-975f-3b91d725dc9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|start_header_id|>assistant<|end_header_id|>\\n\\nI'm so sorry to hear that you're going through this. Losing a pet is never easy, and it's normal to feel sad and emotional.\\n\\nCan you tell me a little bit more about your dog and what they meant to you? Sometimes sharing your feelings with someone who listens can help you process your emotions.\\n\\nIt might be helpful to acknowledge that your feelings are valid and that it's okay to grieve the loss of your pet. It's a significant loss, and it's normal to take time to adjust to the new reality.\\n\\nIf you feel comfortable, what was your dog's personality like? What were some of your favorite memories together?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26751614-c3b6-48d1-a349-c4c8726eff94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm so sorry to hear that you're going through this. Losing a pet is never easy, and it's normal to feel sad and emotional. Can you tell me a little bit more about your dog and what they meant to you? Sometimes sharing your feelings with someone who listens can help you process your emotions. It might be helpful to acknowledge that your feelings are valid and that it's okay to grieve the loss of your pet. It's a significant loss, and it's normal to take time to adjust to the new reality. If you feel comfortable, what was your dog's personality like? What were some of your favorite memories together?\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1189d33b-9245-46a5-8dbd-9e6ec17c353f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Congratulations on your new promotion! That's a tremendous accomplishment! It's understandable that your colleagues might be adjusting to the change, especially if they're used to working under your guidance as an individual contributor. It's usual for people to feel uncertain or even resentful when someone takes a new role, as it can disrupt the workflow and dynamics within the team. As their manager, you have a great opportunity to set a positive tone and build trust with your colleagues. Here are some suggestions to help you navigate this situation: 1. **Communicate openly and transparently**: Hold an open-door meeting or a team-wide discussion to address any concerns or questions your colleagues might have. Be approachable and listen attentively to their thoughts and feelings.\\n2. **Emphasize collaboration and teamwork**: Let your team know that your promote doesn't mean you'll be distancing yourself from them. Confirm that you're committed to working closely with them to achieve shared goals and celebrate individual contributions.\\n3. **Recognize and respect their roles**: Acknowledge the value and importance of each person's role, and make sure everyone understands how their tasks fit into the bigger picture. Encourage open communication and feedback to foster a sense of inclusivity.\\n4. **Foster a positive work\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135df057-ba6e-4580-9abb-93678dc44d6d",
   "metadata": {},
   "source": [
    "### Install Flash Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb57b971-1851-41db-8ba9-8e2eee77eea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not have nvcc at beginning, then ask to install nvidia-cuda-toolkit, then the cuda version becomes 11.5 < 11.6\n",
    "# flash-attention supports 11.6 and above.\n",
    "# Then update cuda to 11.8 by install nvidia cuda driver, but afterwards the GPU driver returns back to an old version\n",
    "# Then start to install a new nvidia driver by software & updates app in the ubuntu, like last time, in the updated drivers\n",
    "# This time we update to 545 though the latest  version should be 550.\n",
    "# Anyway, let's just use current version to make it run at least. Flash attention works now, good."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
