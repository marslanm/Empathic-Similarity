{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d9aa197-9952-4b1a-9659-e9674e6415fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import GenerationConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1845da2a-afc4-46ec-9186-0a240fb4a5bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.get_logger(\"transformers\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a151465-3390-4c14-b44a-1de25aa946e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = './empathic-stories/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d909484-e1bb-4252-ac06-deef97ad4cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{data_path}/PAIRS (train).csv')\n",
    "dev_df = pd.read_csv(f'{data_path}/PAIRS (dev).csv')\n",
    "test_df = pd.read_csv(f'{data_path}/PAIRS (test).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a5512fe-4e3d-470f-8c3e-07fc44e2ed1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "label_columns = \"similarity_empathy_human_AGG\tsimilarity_event_human_AGG\tsimilarity_emotion_human_AGG\tsimilarity_moral_human_AGG\".split()[::-1]\n",
    "text_preprocess = lambda x: x.strip().replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dd51d85-9135-47b9-9469-7124bc1885cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7f0f2fc-d96b-47d3-bb90-578860b6646e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl.commands.cli_utils import SftScriptArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c39984fb-2d2c-42a9-949b-b86054077012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "story_in_use = 'summary'\n",
    "label_in_use = 'empathy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b980059-f1a6-4ed6-901f-4609deddcd99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "text_columns = {\n",
    "    \"summary\": [\"story_A_summary\", \"story_B_summary\"],\n",
    "    \"full\": [\"story_A\", \"story_B\"],\n",
    "}.get(story_in_use)\n",
    "label_column = {\n",
    "    \"empathy\": [\"similarity_empathy_human_AGG\"],\n",
    "    \"event\": [\"similarity_event_human_AGG\"],\n",
    "    \"emotion\": [\"similarity_emotion_human_AGG\"],\n",
    "    \"moral\": [\"similarity_moral_human_AGG\"],\n",
    "    \"all\": label_columns,\n",
    "}.get(label_in_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d2394c4-a2e1-42ed-8db0-7401343b922e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score_conversion_funcs = {\n",
    "    \"none\": lambda x:x,\n",
    "    \"original_paper\": lambda x: x / 4,\n",
    "    \"01_continue\": lambda x:(x - 1) / 3,\n",
    "}\n",
    "score_recover_funcs = {\n",
    "    \"none\": lambda x:x,\n",
    "    \"original_paper\": lambda x: x * 4,\n",
    "    \"01_continue\": lambda x:(x * 3) + 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "711cbbfc-5359-4caf-9795-27d0dd618fba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose score conversion method\n",
    "score_conversion_in_use = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "660a0fed-93ee-40a7-8d0f-61574b69797f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score_conversion_func = score_conversion_funcs.get(score_conversion_in_use)\n",
    "score_recover_func = score_recover_funcs.get(score_conversion_in_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75e34b34-70e8-4777-925c-907b5029678e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_data(df, text_pps, score_conversion_funcs):\n",
    "    required_columns = text_columns + label_column\n",
    "    score_names = [f\"score_{i}\" for i in range(len(label_column))]\n",
    "    df = df[required_columns].rename(\n",
    "        columns={\n",
    "            k: v\n",
    "            for k, v in zip(\n",
    "                required_columns, [\"sentence1\", \"sentence2\"] + score_names\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    for i in [1, 2]:\n",
    "        df[f\"sentence{i}\"] = df[f\"sentence{i}\"].apply(text_pps)\n",
    "    for i in range(len(label_column)):\n",
    "        df[f\"score_{i}\"] = score_conversion_funcs(df[f\"score_{i}\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efc408c7-5fc2-4130-afb8-99084dac085b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = create_data(train_df, text_preprocess, score_conversion_func)\n",
    "dev_df = create_data(dev_df, text_preprocess, score_conversion_func)\n",
    "test_df = create_data(test_df, text_preprocess, score_conversion_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f326cf35-253a-4c89-ba2c-43361b144a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36cead8e-56a1-4aab-b4c1-429a7b673d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimized_similarity_system_prompt = \"\"\"Rate the extent to which you agree with the statement \"the narrators of the two stories would empathize with each other.\" We define empathy as feeling, understanding, and relating to what another person is experiencing. Note that it is possible to have empathy even without sharing the exact same experience or circumstance. Importantly, for two stories to be empathetically similar, both narrators should be able to empathize with each other (if narrator A’s story was shared in response to narrator B’s story, narrator B would empathize with narrator A and vice versa). Give your answer on a scale from 1-4 (1-not at all, 2-not so much, 3-very much, 4-extremely), with 0.5 increments in each level between 1-4 are allowed. Please only return the score without any explanation.\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fad2f75-5655-4430-820c-9c5a15cba821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "optimized_similarity_user_prompt = \"\"\"\n",
    "### Narrative A:\n",
    "{story_a}\n",
    "\n",
    "### Narrative B:\n",
    "{story_b}\n",
    "\n",
    "### Similarity Score:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93b20a1e-b09f-4052-b114-62a088b59161",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf9b43d0-b292-42c9-b769-9236b53c1c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./fullpsft_llama3-emp_2gpus-summary-full_text_loss/\", padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19a7f175-2937-4e65-98f3-45e31ee64338",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def promptify(x1, x2, labels):\n",
    "    user_input = optimized_similarity_user_prompt.format(\n",
    "        story_a=x1, story_b=x2\n",
    "    ).strip()\n",
    "    label = ' '.join([str(score_conversion_func(l)) for l in labels])\n",
    "    model_input = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": optimized_similarity_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_input},\n",
    "            {\"role\": \"assistant\", \"content\": label},\n",
    "        ],\n",
    "        tokenize=False,\n",
    "    )\n",
    "    return {\"text\": model_input}\n",
    "\n",
    "def create_dataset(df, concat_reverse=False, shuffle=False):\n",
    "    score_columns = [f\"score_{i}\" for i in range(len(label_column))]\n",
    "    dataset = (\n",
    "        Dataset.from_pandas(df)\n",
    "        .map(lambda x: promptify(x[\"sentence1\"], x[\"sentence2\"], [x[s] for s in score_columns]))\n",
    "        .remove_columns([\"sentence1\", \"sentence2\"] + score_columns)\n",
    "    )\n",
    "    if concat_reverse:\n",
    "        _ = (\n",
    "            Dataset.from_pandas(df)\n",
    "            .map(lambda x: promptify(x[\"sentence2\"], x[\"sentence1\"], [x[s] for s in score_columns]))\n",
    "            .remove_columns([\"sentence1\", \"sentence2\"] + score_columns)\n",
    "        )\n",
    "        dataset = datasets.concatenate_datasets([dataset, _])\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle()\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33aa4c1f-9b3e-4673-b703-3bfed028fd6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c11a84e0c0948db8399ebfec714d37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f0b3131db747459e156bad89adae4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9726169670a4e5e951d85c73ab1b16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115a66b45e5146fc91fb2897cb3da79b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = create_dataset(train_df, concat_reverse=True, shuffle=True)\n",
    "dev_dataset = create_dataset(dev_df, concat_reverse=False, shuffle=False)\n",
    "test_dataset = create_dataset(test_df, concat_reverse=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4777d5c-1214-4fa0-b63f-74135e028616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfd357be-94de-479f-a326-92e109e5c847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8cf83da73d44988cb565afdc6b94fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('./fullpsft_llama3-emp_2gpus-summary-full_text_loss/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0498f852-2e93-4957-8fba-556c5cc9b448",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE='cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27fc51ef-2367-4270-8ca5-d9075b62d61b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.to(DEVICE).to(torch.bfloat16).eval()\n",
    "model.generation_config.pad_token_id=tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a48f690-3199-4d23-a843-f9b21757620d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained('meta-llama/Meta-Llama-3-8B-Instruct')\n",
    "tokenizer.pad_token_id = tokenizer.added_tokens_encoder[\n",
    "    \"<|reserved_special_token_0|>\"\n",
    "]\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "generation_config.max_length = 512\n",
    "\n",
    "generation_config.temperature = 0.0\n",
    "generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "generation_config.do_sample = False\n",
    "model.generation_config.pad_token_id=tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3d3d98e-f2f0-45b0-81db-0dd182ec79a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_predict_batch(samples):\n",
    "    model_inputs, labels = zip(*[ sample.split('<|start_header_id|>assistant<|end_header_id|>') for sample in samples['text']])\n",
    "    model_inputs = [ x+'<|start_header_id|>assistant<|end_header_id|>' for x in model_inputs]\n",
    "    # model_input, label = sample['text'].split('<|start_header_id|>assistant<|end_header_id|>')\n",
    "    labels = [ label.strip().split('<')[0] for label in labels]\n",
    "    model_inputs = tokenizer(model_inputs, add_special_tokens=False, return_tensors='pt', padding=True)\n",
    "    \n",
    "    response = model.generate(**{ k:v.to(DEVICE) for k,v in model_inputs.items()}, generation_config=generation_config, pad_token_id=tokenizer.eos_token_id)\n",
    "    return model_inputs, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c020c15c-f3fc-49ac-99ae-ae5b9c7e375a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d5ed028-4622-40c3-9508-1697acd73953",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minghanwu/.conda/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/minghanwu/.conda/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_inputs, response = test_predict_batch(dev_dataset[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "87c1dc6d-0989-42c1-92bc-971b6ce24bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(samples):\n",
    "    model_inputs, labels = zip(*[ sample.split('<|start_header_id|>assistant<|end_header_id|>') for sample in samples['text']])\n",
    "    model_inputs = [ x+'<|start_header_id|>assistant<|end_header_id|>' for x in model_inputs]\n",
    "    labels = [ label.strip().split('<')[0] for label in labels]\n",
    "    model_inputs = tokenizer(model_inputs, add_special_tokens=False, return_tensors='pt', padding=True)\n",
    "    \n",
    "    response = model.generate(**{ k:v.to(DEVICE) for k,v in model_inputs.items()}, generation_config=generation_config, pad_token_id=tokenizer.eos_token_id)\n",
    "    predicted_scores = [ tokenizer.decode(response[i][model_inputs.input_ids[i].masked_select(model_inputs.input_ids[i]!=tokenizer.pad_token_id).shape[0]:]).split('\\n')[-1].split('<')[0] for i in range(len(labels))]\n",
    "    predicted_scores = [ multi_scores.split() for multi_scores in predicted_scores]\n",
    "    labels = [ multi_labels.split() for multi_labels in labels]\n",
    "    return predicted_scores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09150672-780d-4454-88fc-32f1a02103b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_df.groupby('score_0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2c5fe49-4f4a-4aab-8516-797531930c20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict_batch(dev_dataset[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d4854a18-04cc-44b9-9f64-2357bd2e0b29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.generation_config.padding_side='left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f771ee77-0396-4e0d-9ee8-9b518cec8375",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "41edcdca-611c-4f04-8b27-543a595bd214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_prediction(dataset, bsz=20):\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    for i in tqdm.tqdm(range(len(dataset) // bsz + 1)):\n",
    "        batch = dataset[i*bsz:(i+1)*bsz]\n",
    "        if len(batch['text']) == 0:\n",
    "            break\n",
    "        predicted_score, label = predict_batch(batch)\n",
    "        predictions.extend(predicted_score)\n",
    "        labels.extend(label)\n",
    "    bin_labels = [ [int(float(x)>2.5) for x in multi_labels] for multi_labels in labels]\n",
    "    bin_prediction = [ [int(float(x)>2.5) for x in multi_predictions ] for multi_predictions in predictions]\n",
    "    float_predictions = [ [float(x) for x in multi_predictions ] for multi_predictions in predictions]\n",
    "    float_labels = [ [float(x) for x in multi_labels ] for multi_labels in labels]\n",
    "    return float_predictions, float_labels, bin_prediction, bin_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a789612-0871-40f2-b166-bf2ecb26efd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# float_predictions, float_labels, bin_prediction, bin_labels = make_prediction(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2f4bd3b-f323-4bf5-8f3a-09e044c6b86b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(dataset):\n",
    "    float_predictions, float_labels, bin_prediction, bin_labels = make_prediction(dataset)\n",
    "    cls_dfs = [ pd.DataFrame(classification_report(bin_label, bin_prediction, output_dict=True)).T for bin_label, bin_prediction in zip(zip(*bin_prediction), zip(*bin_labels))]\n",
    "    spearmanrs = [scipy.stats.spearmanr(np.array(float_label),np.array(float_prediction)).statistic for float_label, float_prediction in zip(zip(*float_predictions), zip(*float_labels))]\n",
    "    pearsonrs = [ scipy.stats.pearsonr(np.array(float_label),np.array(float_prediction)).statistic  for float_label, float_prediction in zip(zip(*float_predictions), zip(*float_labels))]\n",
    "    mses = [  np.square(np.subtract(np.array(float_label),np.array(float_prediction))).mean()  for float_label, float_prediction in zip(zip(*float_predictions), zip(*float_labels))]\n",
    "    sp_df = pd.DataFrame({'S': spearmanrs, 'P': pearsonrs, 'M':mses})\n",
    "    return cls_dfs, sp_df, float_predictions, float_labels, bin_prediction, bin_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b77217c-1163-4984-8d49-04b220cf26b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|██████████████████████████████████████████████████████████████████████▊              | 5/6 [00:07<00:01,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "dev_cls_df, dev_sp_df, predicted_dev_scores, dev_score, dev_bin_predictions, dev_bin_labels=evaluate(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "18faad9d-6f01-46ed-9ffa-ddb45a08c060",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.596774</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>49.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.584270</td>\n",
       "      <td>51.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.640492</td>\n",
       "      <td>0.632453</td>\n",
       "      <td>0.625468</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.641367</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.624644</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score  support\n",
       "0              0.596774  0.755102  0.666667    49.00\n",
       "1              0.684211  0.509804  0.584270    51.00\n",
       "accuracy       0.630000  0.630000  0.630000     0.63\n",
       "macro avg      0.640492  0.632453  0.625468   100.00\n",
       "weighted avg   0.641367  0.630000  0.624644   100.00"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_cls_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d8a23d4-66bd-44ba-8020-7ba4be8005c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      " & precision & recall & f1-score & support \\\\\n",
      "\\midrule\n",
      "0 & 0.597 & 0.755 & 0.667 & 49.000 \\\\\n",
      "1 & 0.684 & 0.510 & 0.584 & 51.000 \\\\\n",
      "accuracy & 0.630 & 0.630 & 0.630 & 0.630 \\\\\n",
      "macro avg & 0.640 & 0.632 & 0.625 & 100.000 \\\\\n",
      "weighted avg & 0.641 & 0.630 & 0.625 & 100.000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dev_cls_df[0].to_latex(float_format=lambda x:f'{x:.3f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf325802-9140-4f25-8831-fe7f6cc4b451",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      " & Spearman & Pearson & MSE \\\\\n",
      "\\midrule\n",
      "0 & 0.209 & 0.189 & 0.627 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dev_sp_df.rename(columns={'S':'Spearman','P':\"Pearson\", 'M':'MSE'}).to_latex(float_format=lambda x:f'{x:.3f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "adbd5744-7a4e-4073-a7c8-f0479aa680b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/21 [00:00<?, ?it/s]/home/minghanwu/.conda/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/minghanwu/.conda/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      " 95%|███████████████████████████████████████████████████████████████████████████████    | 20/21 [00:30<00:01,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "test_cls_df, test_sp_df, predicted_test_scores, test_score, test_bin_predictions, test_bin_labels=evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c7f3f955-3bbf-4c1e-a8c1-17acf495e7a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      " & precision & recall & f1-score & support \\\\\n",
      "\\midrule\n",
      "0 & 0.431 & 0.494 & 0.460 & 170.000 \\\\\n",
      "1 & 0.580 & 0.517 & 0.547 & 230.000 \\\\\n",
      "accuracy & 0.507 & 0.507 & 0.507 & 0.507 \\\\\n",
      "macro avg & 0.506 & 0.506 & 0.504 & 400.000 \\\\\n",
      "weighted avg & 0.517 & 0.507 & 0.510 & 400.000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_cls_df[0].to_latex(float_format=lambda x:f'{x:.3f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82c83a51-7bfe-445a-890b-8e2bd6be64b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.430769</td>\n",
       "      <td>0.494118</td>\n",
       "      <td>0.460274</td>\n",
       "      <td>170.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.580488</td>\n",
       "      <td>0.517391</td>\n",
       "      <td>0.547126</td>\n",
       "      <td>230.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.507500</td>\n",
       "      <td>0.507500</td>\n",
       "      <td>0.507500</td>\n",
       "      <td>0.5075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.505629</td>\n",
       "      <td>0.505754</td>\n",
       "      <td>0.503700</td>\n",
       "      <td>400.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.516857</td>\n",
       "      <td>0.507500</td>\n",
       "      <td>0.510214</td>\n",
       "      <td>400.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score   support\n",
       "0              0.430769  0.494118  0.460274  170.0000\n",
       "1              0.580488  0.517391  0.547126  230.0000\n",
       "accuracy       0.507500  0.507500  0.507500    0.5075\n",
       "macro avg      0.505629  0.505754  0.503700  400.0000\n",
       "weighted avg   0.516857  0.507500  0.510214  400.0000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cls_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2a8af5da-e218-4d31-9e6c-46ecef7c686b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      " & precision & recall & f1-score & support \\\\\n",
      "\\midrule\n",
      "0 & 0.431 & 0.494 & 0.460 & 170.000 \\\\\n",
      "1 & 0.580 & 0.517 & 0.547 & 230.000 \\\\\n",
      "accuracy & 0.507 & 0.507 & 0.507 & 0.507 \\\\\n",
      "macro avg & 0.506 & 0.506 & 0.504 & 400.000 \\\\\n",
      "weighted avg & 0.517 & 0.507 & 0.510 & 400.000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_cls_df[0].to_latex(float_format=lambda x:f'{x:.3f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "892c9b12-3043-4479-88f1-1823344d9ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      " & Spearman & Pearson & MSE \\\\\n",
      "\\midrule\n",
      "0 & 0.028 & 0.039 & 0.773 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_sp_df.rename(columns={'S':'Spearman','P':\"Pearson\", 'M':'MSE'}).to_latex(float_format=lambda x:f'{x:.3f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aa446a0c-d07d-473e-8ba9-6e2ecb666701",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      " & Spearman & Pearson & MSE \\\\\n",
      "\\midrule\n",
      "0 & 0.003 & 0.005 & 0.811 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_sp_df.rename(columns={'S':'Spearman','P':\"Pearson\", 'M':'MSE'}).to_latex(float_format=lambda x:f'{x:.3f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318a2b40-4086-484c-8a2d-90bdc0ca22d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
