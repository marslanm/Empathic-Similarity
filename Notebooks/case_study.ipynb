{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94f1ca3-39d4-46bc-b41d-98a1a950c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def eval_sts(labels, y_pred):\n",
    "    eval_pearson, _ = pearsonr(labels, y_pred)\n",
    "    eval_spearman, _ = spearmanr(labels, y_pred)\n",
    "    mse = np.square(np.subtract(labels, y_pred)).mean()\n",
    "\n",
    "    metrics = {\n",
    "        \"Pearson\": round(eval_pearson, 3),\n",
    "        \"Spearman\": round(eval_spearman, 3),\n",
    "        \"MSE\": round(mse, 2)\n",
    "    }\n",
    "    \n",
    "    # print(f\"Pearson correlation {metrics['Pearson']}, Spearman correlation {metrics['Spearman']}, MSE {metrics['MSE']}\")\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def eval_nli(y_true, y_pred, average=\"macro\"):\n",
    "    precision, recall, F1, support = precision_recall_fscore_support(y_true, y_pred, average=average)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": round(accuracy, 3),\n",
    "        \"precision\": round(precision, 3),\n",
    "        \"recall\": round(recall, 3),\n",
    "        \"F1\": round(F1, 3),\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec8e362-8c25-42af-ad10-0e4b01902680",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f36aa2a0-b519-40d2-932b-fcbb6ee4d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = pd.read_csv(\"./data/Test_set_scores.csv\")\n",
    "df_v = pd.read_csv(\"./data/Validation_set_Scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d3ac844-76fb-41fb-912a-9205be62e8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['similarity_empathy_human_AGG', 'similarity_event_human_AGG',\n",
       "       'similarity_emotion_human_AGG', 'similarity_moral_human_AGG',\n",
       "       'Unnamed: 4', 'Unnamed: 5', 'SBERT_score', 'BART_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2260222-c395-443b-a695-fd48337ea34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity_empathy_human_AGG</th>\n",
       "      <th>similarity_event_human_AGG</th>\n",
       "      <th>similarity_emotion_human_AGG</th>\n",
       "      <th>similarity_moral_human_AGG</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>SBERT_score</th>\n",
       "      <th>BART_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.941957</td>\n",
       "      <td>0.334691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.408138</td>\n",
       "      <td>0.706342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.902103</td>\n",
       "      <td>0.996153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.032713</td>\n",
       "      <td>0.915401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   similarity_empathy_human_AGG  similarity_event_human_AGG  \\\n",
       "0                           2.5                         1.0   \n",
       "1                           1.5                         2.0   \n",
       "2                           2.0                         2.0   \n",
       "3                           4.0                         2.0   \n",
       "\n",
       "   similarity_emotion_human_AGG  similarity_moral_human_AGG  Unnamed: 4  \\\n",
       "0                           2.0                         2.5         NaN   \n",
       "1                           2.0                         1.5         NaN   \n",
       "2                           2.0                         2.0         NaN   \n",
       "3                           2.5                         2.5         NaN   \n",
       "\n",
       "   Unnamed: 5  SBERT_score  BART_score  \n",
       "0         NaN     1.941957    0.334691  \n",
       "1         NaN     2.408138    0.706342  \n",
       "2         NaN     0.902103    0.996153  \n",
       "3         NaN     2.032713    0.915401  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_v[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "840ffb70-0b67-4c31-ae22-27b504dc38cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation set\n",
      "SBERT_score empathy 0.239\n",
      "SBERT_score event 0.378\n",
      "SBERT_score emotion 0.296\n",
      "SBERT_score moral 0.263\n",
      "\n",
      "\n",
      "BART_score empathy 0.35\n",
      "BART_score event 0.385\n",
      "BART_score emotion 0.25\n",
      "BART_score moral 0.335\n",
      "\n",
      "\n",
      "test set\n",
      "SBERT empathy 0.346\n",
      "SBERT event 0.439\n",
      "SBERT emotion 0.381\n",
      "SBERT moral 0.365\n",
      "\n",
      "\n",
      "BART empathy 0.333\n",
      "BART event 0.417\n",
      "BART emotion 0.332\n",
      "BART moral 0.381\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, df in zip([\"validation set\", \"test set\"], [df_v, df_t]):\n",
    "    print(k)\n",
    "    if k == \"validation set\":\n",
    "        models = ['SBERT_score', 'BART_score']\n",
    "    elif k == \"test set\":\n",
    "        models = ['SBERT', 'BART']\n",
    "    for model in models:\n",
    "        for human in ['similarity_empathy_human_AGG', 'similarity_event_human_AGG',\\\n",
    "                  'similarity_emotion_human_AGG', 'similarity_moral_human_AGG']:\n",
    "            gold = df[human].to_list()\n",
    "            pred = df[model].to_list()\n",
    "            metrics = eval_sts(labels = gold, y_pred = pred)\n",
    "            print(model, human.split(\"_\")[-3], metrics[\"Pearson\"])\n",
    "        print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606590ce-e71a-416e-8a70-bd377f0bb428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
