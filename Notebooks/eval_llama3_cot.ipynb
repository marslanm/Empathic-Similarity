{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d9aa197-9952-4b1a-9659-e9674e6415fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import GenerationConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1845da2a-afc4-46ec-9186-0a240fb4a5bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.get_logger(\"transformers\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a151465-3390-4c14-b44a-1de25aa946e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = './empathic-stories/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d909484-e1bb-4252-ac06-deef97ad4cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{data_path}/PAIRS (train).csv')\n",
    "dev_df = pd.read_csv(f'{data_path}/PAIRS (dev).csv')\n",
    "test_df = pd.read_csv(f'{data_path}/PAIRS (test).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a5512fe-4e3d-470f-8c3e-07fc44e2ed1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "label_columns = \"similarity_empathy_human_AGG\tsimilarity_event_human_AGG\tsimilarity_emotion_human_AGG\tsimilarity_moral_human_AGG\".split()[::-1]\n",
    "text_preprocess = lambda x: x.strip().replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dd51d85-9135-47b9-9469-7124bc1885cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c39984fb-2d2c-42a9-949b-b86054077012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "story_in_use = 'summary'\n",
    "label_in_use = 'empathy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b980059-f1a6-4ed6-901f-4609deddcd99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "text_columns = {\n",
    "    \"summary\": [\"story_A_summary\", \"story_B_summary\"],\n",
    "    \"full\": [\"story_A\", \"story_B\"],\n",
    "}.get(story_in_use)\n",
    "label_column = {\n",
    "    \"empathy\": [\"similarity_empathy_human_AGG\"],\n",
    "    \"event\": [\"similarity_event_human_AGG\"],\n",
    "    \"emotion\": [\"similarity_emotion_human_AGG\"],\n",
    "    \"moral\": [\"similarity_moral_human_AGG\"],\n",
    "    \"all\": label_columns,\n",
    "}.get(label_in_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d2394c4-a2e1-42ed-8db0-7401343b922e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score_conversion_funcs = {\n",
    "    \"none\": lambda x:x,\n",
    "    \"original_paper\": lambda x: x / 4,\n",
    "    \"01_continue\": lambda x:(x - 1) / 3,\n",
    "}\n",
    "score_recover_funcs = {\n",
    "    \"none\": lambda x:x,\n",
    "    \"original_paper\": lambda x: x * 4,\n",
    "    \"01_continue\": lambda x:(x * 3) + 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "711cbbfc-5359-4caf-9795-27d0dd618fba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose score conversion method\n",
    "score_conversion_in_use = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "660a0fed-93ee-40a7-8d0f-61574b69797f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score_conversion_func = score_conversion_funcs.get(score_conversion_in_use)\n",
    "score_recover_func = score_recover_funcs.get(score_conversion_in_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75e34b34-70e8-4777-925c-907b5029678e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_data(df, text_pps, score_conversion_funcs, analysis_df):\n",
    "    required_columns = text_columns + label_column\n",
    "    score_names = [f\"score_{i}\" for i in range(len(label_column))]\n",
    "    df = df[required_columns].rename(\n",
    "        columns={\n",
    "            k: v\n",
    "            for k, v in zip(\n",
    "                required_columns, [\"sentence1\", \"sentence2\"] + score_names)\n",
    "        }\n",
    "    )\n",
    "    for i in [1, 2]:\n",
    "        df[f\"sentence{i}\"] = df[f\"sentence{i}\"].apply(text_pps)\n",
    "    for i in range(len(label_column)):\n",
    "        df[f\"score_{i}\"] = score_conversion_funcs(df[f\"score_{i}\"])\n",
    "    return pd.concat([df, analysis_df],axis=1)[[\"sentence1\", \"sentence2\", \"analysis\"] + score_names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b571b3b7-89ea-49b7-9650-fa1f58deb2a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "process_analysis = lambda d: '\\n\\n'.join(d.split('\\n\\n')[np.argmax(list(map(lambda x: \"Thematic Similarities\" in x,d.split('\\n\\n')))):])\n",
    "analysis_path = './data/'\n",
    "train_analysis = pd.read_json(f\"{analysis_path}/train_explanations_{story_in_use}.json\").rename(columns={0: \"analysis\"})\n",
    "train_analysis['analysis'] = train_analysis.apply(lambda x:process_analysis(x['analysis']),axis=1)\n",
    "dev_analysis = pd.read_json(f\"{analysis_path}/dev_explanations_{story_in_use}.json\").rename(columns={0: \"analysis\"})\n",
    "dev_analysis['analysis'] = dev_analysis.apply(lambda x:process_analysis(x['analysis']),axis=1)\n",
    "test_analysis = pd.read_json(f\"{analysis_path}/test_explanations_{story_in_use}.json\").rename(columns={0: \"analysis\"})\n",
    "test_analysis['analysis'] = test_analysis.apply(lambda x:process_analysis(x['analysis']),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efc408c7-5fc2-4130-afb8-99084dac085b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_df = create_data(train_df, text_preprocess, score_conversion_func, train_analysis)\n",
    "dev_df = create_data(dev_df, text_preprocess, score_conversion_func, dev_analysis)\n",
    "test_df = create_data(test_df, text_preprocess, score_conversion_func, test_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f326cf35-253a-4c89-ba2c-43361b144a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36cead8e-56a1-4aab-b4c1-429a7b673d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimized_similarity_system_prompt = \"\"\"\n",
    "We define empathy as feeling, understanding, and relating to what another person is experiencing.\n",
    "Note that it is possible to have empathy even without sharing the exact same experience or circumstance.\n",
    "Your task is to measure the empathic similarity of the given two stories.\n",
    "Importantly, for two stories to be empathetically similar, both narrators should be able to empathize with each other (if narrator A’s story was shared in response to narrator B’s story, narrator B would empathize with narrator A and vice versa).\n",
    "Give your answer on a scale from 1-4 (1-not at all, 2-not so much, 3-very much, 4-extremely), with 0.5 increments in each level between 1-4 are allowed.\n",
    "You should first analyze the two stories, and then, return the score in a JSON object.\"\"\".strip()\n",
    "\n",
    "optimized_similarity_user_prompt = \"\"\"\n",
    "## Narrative A:\n",
    "{story_a}\n",
    "\n",
    "## Narrative B:\n",
    "{story_b}\n",
    "\"\"\"\n",
    "\n",
    "optimized_similarity_response_prompt = \"\"\"\n",
    "## Analysis\n",
    "{analysis}\n",
    "\n",
    "## Similarity Score:\n",
    "```\n",
    "{{\"score\": {score}}}\n",
    "```\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93b20a1e-b09f-4052-b114-62a088b59161",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf9b43d0-b292-42c9-b769-9236b53c1c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3feb8359-199e-440d-b3c3-e45d0b07a3ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'left'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19a7f175-2937-4e65-98f3-45e31ee64338",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def promptify(x1, x2, analysis, labels):\n",
    "    user_input = optimized_similarity_user_prompt.format(\n",
    "        story_a=x1, story_b=x2\n",
    "    ).strip()\n",
    "    label = optimized_similarity_response_prompt.format(\n",
    "        analysis=analysis, score=score_recover_func(labels[0])\n",
    "    ).strip()\n",
    "    model_input = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": optimized_similarity_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_input},\n",
    "            {\"role\": \"assistant\", \"content\": label},\n",
    "        ],\n",
    "        tokenize=False,\n",
    "    )\n",
    "    return {\"text\": model_input}\n",
    "\n",
    "def create_dataset(df, concat_reverse=False, shuffle=False):\n",
    "    score_columns = [f\"score_{i}\" for i in range(len(label_column))]\n",
    "    dataset = (\n",
    "        Dataset.from_pandas(df)\n",
    "        .map(lambda x: promptify(x[\"sentence1\"], x[\"sentence2\"], x['analysis'], [x[s] for s in score_columns]))\n",
    "        .remove_columns([\"sentence1\", \"sentence2\", \"analysis\"] + score_columns)\n",
    "    )\n",
    "    if concat_reverse:\n",
    "        _ = (\n",
    "            Dataset.from_pandas(df)\n",
    "            .map(lambda x: promptify(x[\"sentence2\"], x[\"sentence1\"], x['analysis'], [x[s] for s in score_columns]))\n",
    "            .remove_columns([\"sentence1\", \"sentence2\", \"analysis\"] + score_columns)\n",
    "        )\n",
    "        dataset = datasets.concatenate_datasets([dataset, _])\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33aa4c1f-9b3e-4673-b703-3bfed028fd6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e061edce0ae44b1bb02d7e3bf7fc27f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a9bd97eb90451b9614f1ee8776b3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7160c4a6614d9188bda8b6cf070314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282918883c6b4bbebd6709e340840ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = create_dataset(train_df, concat_reverse=True, shuffle=True)\n",
    "dev_dataset = create_dataset(dev_df, concat_reverse=False, shuffle=False)\n",
    "test_dataset = create_dataset(test_df, concat_reverse=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4777d5c-1214-4fa0-b63f-74135e028616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfd357be-94de-479f-a326-92e109e5c847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95640b84a6f34dbf9198b208f2336590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained('./sft_llama3-emp_2gpus-summary-cot-3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27fc51ef-2367-4270-8ca5-d9075b62d61b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.cuda(0).to(torch.bfloat16).eval()\n",
    "model = model.base_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a48f690-3199-4d23-a843-f9b21757620d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained('meta-llama/Meta-Llama-3-8B-Instruct')\n",
    "tokenizer.pad_token_id = tokenizer.added_tokens_encoder[\n",
    "    \"<|reserved_special_token_0|>\"\n",
    "]\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "generation_config.max_length = 2048\n",
    "\n",
    "generation_config.temperature = 0.0\n",
    "generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "generation_config.do_sample = False\n",
    "model.generation_config.pad_token_id=tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3d3d98e-f2f0-45b0-81db-0dd182ec79a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_batch(samples):\n",
    "    model_inputs, labels = list(zip(*[ sample.split('<|start_header_id|>assistant<|end_header_id|>') for sample in samples['text']]))\n",
    "    model_inputs = [ x+'<|start_header_id|>assistant<|end_header_id|>' for x in model_inputs]\n",
    "    labels = [ label.strip() for label in labels]\n",
    "    model_inputs = tokenizer(model_inputs, add_special_tokens=False, return_tensors='pt', padding=True)\n",
    "    \n",
    "    response = model.generate(**{ k:v.cuda(0) for k,v in model_inputs.items()}, generation_config=generation_config, pad_token_id=tokenizer.eos_token_id)\n",
    "    predictions = [ tokenizer.decode(result[result!=tokenizer.pad_token_id]) for result in response]\n",
    "    # predictions = [ tokenizer.decode(response[i][model_inputs.input_ids[i].masked_select(model_inputs.input_ids[i]!=tokenizer.pad_token_id).shape[0]:], skip_special_tokens=True) for i in range(len(labels))]\n",
    "    # predicted_scores = [ multi_scores.split() for multi_scores in predicted_scores]\n",
    "    # labels = [ multi_labels.split() for multi_labels in labels]\n",
    "    return predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72a39efb-bae8-425e-aa9e-33a8efcd2d97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predictions, labels = predict_batch(dev_dataset[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83256765-44b3-4910-b4a1-0490f1813ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fa3f108-4dac-458c-abae-fbf4aac68f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_equal = lambda x: all([ x[0] == z for z in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10da296c-b000-4937-944c-305015cf7b33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse(text):\n",
    "    text = text.split('<|start_header_id|>assistant<|end_header_id|>')[-1]\n",
    "    if len(text.split('```')) == 3:\n",
    "        predicted_score = list(eval(text.split('```')[1]).values())[0]\n",
    "        return predicted_score\n",
    "    else:\n",
    "        possible_values = re.findall('(1.0|1.5|2.0|2.5|3.0|3.5|4.0)', text)\n",
    "        if len(possible_values)>0:\n",
    "            all_values = [ eval(x) for x in possible_values]\n",
    "            if all_equal(all_values):\n",
    "                return all_values[0]\n",
    "            else:\n",
    "                return all_values[-1]\n",
    "        else:\n",
    "            print(text)\n",
    "            return 1.0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4854a18-04cc-44b9-9f64-2357bd2e0b29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.generation_config.padding_side='left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f771ee77-0396-4e0d-9ee8-9b518cec8375",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41edcdca-611c-4f04-8b27-543a595bd214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_prediction(dataset, bsz=20):\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    for i in tqdm.tqdm(range(len(dataset) // bsz + 1)):\n",
    "        batch = dataset[i*bsz:(i+1)*bsz]\n",
    "        if len(batch['text']) == 0:\n",
    "            break\n",
    "        predicted_score, label = predict_batch(batch)\n",
    "    # label = [ parse(x) for x in label]\n",
    "    # predicted_score = [ parse(x) for x in predicted_score]\n",
    "        predictions.extend(predicted_score)\n",
    "        labels.extend(label)\n",
    "    # bin_labels = [ [int(float(x)>2.5) for x in multi_labels] for multi_labels in labels]\n",
    "    # bin_prediction = [ [int(float(x)>2.5) for x in multi_predictions ] for multi_predictions in predictions]\n",
    "    # float_predictions = [ [float(x) for x in multi_predictions ] for multi_predictions in predictions]\n",
    "    # float_labels = [ [float(x) for x in multi_labels ] for multi_labels in labels]\n",
    "    return predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2f4bd3b-f323-4bf5-8f3a-09e044c6b86b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(predictions, labels):\n",
    "    bin_labels = [ int(float(x)>2.5) for x in labels]\n",
    "    bin_prediction = [ int(float(x)>2.5) for x in predictions]\n",
    "    float_predictions = predictions\n",
    "    float_labels = labels\n",
    "    cls_dfs = pd.DataFrame(classification_report(bin_labels, bin_prediction, output_dict=True)).T\n",
    "    spearmanr = scipy.stats.spearmanr(np.array(float_labels),np.array(float_predictions)).statistic\n",
    "    pearsonr = scipy.stats.pearsonr(np.array(float_labels),np.array(float_predictions)).statistic\n",
    "    mse = np.square(np.subtract(np.array(float_labels),np.array(float_predictions))).mean()\n",
    "    sp_df = pd.DataFrame({'S': [spearmanr], 'P': [pearsonr], 'M': [mse]})\n",
    "    return cls_dfs, sp_df, float_predictions, float_labels, bin_prediction, bin_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb4acbe-7736-4cdd-b667-3d6b6f403b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a789612-0871-40f2-b166-bf2ecb26efd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]/home/minghanwu/workspace/conda_env/envs/vmt/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/minghanwu/workspace/conda_env/envs/vmt/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      " 83%|█████████████████████████████████████▌       | 5/6 [03:35<00:43, 43.13s/it]\n"
     ]
    }
   ],
   "source": [
    "dev_predictions, dev_labels = make_prediction(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73fda067-8caf-40f6-ae62-3a6896cd57af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dev_predictions = [ parse(x) for x in dev_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0be84c7a-645c-4ab2-acc3-8daac8df5fca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dev_labels = [ parse(x) for x in dev_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b77217c-1163-4984-8d49-04b220cf26b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dev_cls_df, dev_sp_df, predicted_dev_scores, dev_score, dev_bin_predictions, dev_bin_labels=evaluate(dev_predictions, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b253f464-8de3-4de8-b863-54e41b34731c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dev_cls_df.to_csv('./results/dev_cls.csv',float_format=lambda x:f'{x:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0dba2d41-d19e-44e9-9da7-b8ba98e5fa7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.829787</td>\n",
       "      <td>0.629032</td>\n",
       "      <td>0.715596</td>\n",
       "      <td>62.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.659341</td>\n",
       "      <td>38.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.697912</td>\n",
       "      <td>0.709253</td>\n",
       "      <td>0.687468</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.729562</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.694219</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score  support\n",
       "0              0.829787  0.629032  0.715596    62.00\n",
       "1              0.566038  0.789474  0.659341    38.00\n",
       "accuracy       0.690000  0.690000  0.690000     0.69\n",
       "macro avg      0.697912  0.709253  0.687468   100.00\n",
       "weighted avg   0.729562  0.690000  0.694219   100.00"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_cls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a279588-1f43-490f-965a-8519f95de301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dev_sp_df.rename(columns={'S':'Spearman','P':\"Pearson\", 'M':'MSE'}).to_csv('./results/dev_reg.csv',float_format=lambda x:f'{x:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5c7e2e1-a450-41dd-9c10-53ac4fb8af49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      " & precision & recall & f1-score & support \\\\\n",
      "\\midrule\n",
      "0 & 0.830 & 0.629 & 0.716 & 62.000 \\\\\n",
      "1 & 0.566 & 0.789 & 0.659 & 38.000 \\\\\n",
      "accuracy & 0.690 & 0.690 & 0.690 & 0.690 \\\\\n",
      "macro avg & 0.698 & 0.709 & 0.687 & 100.000 \\\\\n",
      "weighted avg & 0.730 & 0.690 & 0.694 & 100.000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dev_cls_df.to_latex(float_format=lambda x:f'{x:.3f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf325802-9140-4f25-8831-fe7f6cc4b451",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>P</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.341992</td>\n",
       "      <td>0.32472</td>\n",
       "      <td>0.6175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          S        P       M\n",
       "0  0.341992  0.32472  0.6175"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_sp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8fa2cf91-6511-4fd1-8af4-c333251dd244",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      " & Spearman & Pearson & MSE \\\\\n",
      "\\midrule\n",
      "0 & 0.342 & 0.325 & 0.618 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dev_sp_df.rename(columns={'S':'Spearman','P':\"Pearson\", 'M':'MSE'}).to_latex(float_format=lambda x:f'{x:.3f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "adbd5744-7a4e-4073-a7c8-f0479aa680b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/21 [00:00<?, ?it/s]/home/minghanwu/workspace/conda_env/envs/vmt/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/minghanwu/workspace/conda_env/envs/vmt/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      " 95%|████████████████████████████████████████▉  | 20/21 [15:42<00:47, 47.14s/it]\n"
     ]
    }
   ],
   "source": [
    "test_predictions, test_labels = make_prediction(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "327a920f-bc0c-4550-a198-cab45e8f25d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_predictions = [ parse(x) for x in test_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2330a4d2-5b2d-4e16-b881-46db01d87408",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_labels = [ parse(x) for x in test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "93b0c71d-a6ad-4f68-9ae1-ef6bfc2257b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_cls_df, test_sp_df, predicted_test_scores, test_score, test_bin_predictions, test_bin_labels=evaluate(test_predictions, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c7f3f955-3bbf-4c1e-a8c1-17acf495e7a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.640845</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.540059</td>\n",
       "      <td>195.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.596899</td>\n",
       "      <td>0.751220</td>\n",
       "      <td>0.665227</td>\n",
       "      <td>205.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.612500</td>\n",
       "      <td>0.612500</td>\n",
       "      <td>0.612500</td>\n",
       "      <td>0.6125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.618872</td>\n",
       "      <td>0.608943</td>\n",
       "      <td>0.602643</td>\n",
       "      <td>400.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.618323</td>\n",
       "      <td>0.612500</td>\n",
       "      <td>0.604208</td>\n",
       "      <td>400.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score   support\n",
       "0              0.640845  0.466667  0.540059  195.0000\n",
       "1              0.596899  0.751220  0.665227  205.0000\n",
       "accuracy       0.612500  0.612500  0.612500    0.6125\n",
       "macro avg      0.618872  0.608943  0.602643  400.0000\n",
       "weighted avg   0.618323  0.612500  0.604208  400.0000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df3b73f9-9c79-40cf-a068-057255b6d5b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_cls_df.to_csv('./results/test_cls.csv',float_format=lambda x:f'{x:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "26ca810d-296e-48df-8528-5b7b3c785af8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      " & precision & recall & f1-score & support \\\\\n",
      "\\midrule\n",
      "0 & 0.641 & 0.467 & 0.540 & 195.000 \\\\\n",
      "1 & 0.597 & 0.751 & 0.665 & 205.000 \\\\\n",
      "accuracy & 0.613 & 0.613 & 0.613 & 0.613 \\\\\n",
      "macro avg & 0.619 & 0.609 & 0.603 & 400.000 \\\\\n",
      "weighted avg & 0.618 & 0.613 & 0.604 & 400.000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_cls_df.to_latex(float_format=lambda x:f'{x:.3f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "892c9b12-3043-4479-88f1-1823344d9ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>P</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.289461</td>\n",
       "      <td>0.298746</td>\n",
       "      <td>0.651875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          S         P         M\n",
       "0  0.289461  0.298746  0.651875"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a05d32f3-9057-4bc0-90c0-c55bff7bc692",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_sp_df.rename(columns={'S':'Spearman','P':\"Pearson\"}).to_csv('./results/test_reg.csv',float_format=lambda x:f'{x:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe7a1ff9-ae45-4aaf-a10b-3d5aca3f5351",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      " & Spearman & Pearson \\\\\n",
      "\\midrule\n",
      "0 & 0.289 & 0.299 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_sp_df.rename(columns={'S':'Spearman','P':\"Pearson\"}).to_latex(float_format=lambda x:f'{x:.3f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b66d3623-4253-4921-9faa-f746e2f13342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fig,axs = plt.subplots(2,2)\n",
    "# axs[0,0].hist(dev_score)\n",
    "# axs[0,1].hist(test_score)\n",
    "# axs[1,0].hist(predicted_dev_scores)\n",
    "# axs[1,1].hist(predicted_test_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vmt",
   "language": "python",
   "name": "vmt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
