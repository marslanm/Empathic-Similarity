{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23371f7f-0ab7-475e-8e4e-ea794d69fc61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d9aa197-9952-4b1a-9659-e9674e6415fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import GenerationConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1845da2a-afc4-46ec-9186-0a240fb4a5bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.get_logger(\"transformers\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a151465-3390-4c14-b44a-1de25aa946e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = './empathic-stories/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d909484-e1bb-4252-ac06-deef97ad4cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{data_path}/PAIRS (train).csv')\n",
    "dev_df = pd.read_csv(f'{data_path}/PAIRS (dev).csv')\n",
    "test_df = pd.read_csv(f'{data_path}/PAIRS (test).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a5512fe-4e3d-470f-8c3e-07fc44e2ed1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "label_columns = \"similarity_empathy_human_AGG\tsimilarity_event_human_AGG\tsimilarity_emotion_human_AGG\tsimilarity_moral_human_AGG\".split()[::-1]\n",
    "text_preprocess = lambda x: x.strip().replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dd51d85-9135-47b9-9469-7124bc1885cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7f0f2fc-d96b-47d3-bb90-578860b6646e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl.commands.cli_utils import SftScriptArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c39984fb-2d2c-42a9-949b-b86054077012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "story_in_use = 'summary'\n",
    "label_in_use = 'empathy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b980059-f1a6-4ed6-901f-4609deddcd99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "text_columns = {\n",
    "    \"summary\": [\"story_A_summary\", \"story_B_summary\"],\n",
    "    \"full\": [\"story_A\", \"story_B\"],\n",
    "}.get(story_in_use)\n",
    "label_column = {\n",
    "    \"empathy\": [\"similarity_empathy_human_AGG\"],\n",
    "    \"event\": [\"similarity_event_human_AGG\"],\n",
    "    \"emotion\": [\"similarity_emotion_human_AGG\"],\n",
    "    \"moral\": [\"similarity_moral_human_AGG\"],\n",
    "    \"all\": label_columns,\n",
    "}.get(label_in_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d2394c4-a2e1-42ed-8db0-7401343b922e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score_conversion_funcs = {\n",
    "    \"none\": lambda x:x,\n",
    "    \"original_paper\": lambda x: x / 4,\n",
    "    \"01_continue\": lambda x:(x - 1) / 3,\n",
    "}\n",
    "score_recover_funcs = {\n",
    "    \"none\": lambda x:x,\n",
    "    \"original_paper\": lambda x: x * 4,\n",
    "    \"01_continue\": lambda x:(x * 3) + 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "711cbbfc-5359-4caf-9795-27d0dd618fba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose score conversion method\n",
    "score_conversion_in_use = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "660a0fed-93ee-40a7-8d0f-61574b69797f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score_conversion_func = score_conversion_funcs.get(score_conversion_in_use)\n",
    "score_recover_func = score_recover_funcs.get(score_conversion_in_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75e34b34-70e8-4777-925c-907b5029678e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_data(df, text_pps, score_conversion_funcs):\n",
    "    required_columns = text_columns + label_column\n",
    "    score_names = [f\"score_{i}\" for i in range(len(label_column))]\n",
    "    df = df[required_columns].rename(\n",
    "        columns={\n",
    "            k: v\n",
    "            for k, v in zip(\n",
    "                required_columns, [\"sentence1\", \"sentence2\"] + score_names\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    for i in [1, 2]:\n",
    "        df[f\"sentence{i}\"] = df[f\"sentence{i}\"].apply(text_pps)\n",
    "    for i in range(len(label_column)):\n",
    "        df[f\"score_{i}\"] = score_conversion_funcs(df[f\"score_{i}\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efc408c7-5fc2-4130-afb8-99084dac085b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = create_data(train_df, text_preprocess, score_conversion_func)\n",
    "dev_df = create_data(dev_df, text_preprocess, score_conversion_func)\n",
    "test_df = create_data(test_df, text_preprocess, score_conversion_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f326cf35-253a-4c89-ba2c-43361b144a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36cead8e-56a1-4aab-b4c1-429a7b673d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimized_similarity_system_prompt = \"\"\"Rate the extent to which you agree with the statement \"the narrators of the two stories would empathize with each other.\" We define empathy as feeling, understanding, and relating to what another person is experiencing. Note that it is possible to have empathy even without sharing the exact same experience or circumstance. Importantly, for two stories to be empathetically similar, both narrators should be able to empathize with each other (if narrator A’s story was shared in response to narrator B’s story, narrator B would empathize with narrator A and vice versa). Give your answer on a scale from 1-4 (1-not at all, 2-not so much, 3-very much, 4-extremely), with 0.5 increments in each level between 1-4 are allowed. Please only return the score without any explanation.\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fad2f75-5655-4430-820c-9c5a15cba821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "optimized_similarity_user_prompt = \"\"\"\n",
    "### Narrative A:\n",
    "{story_a}\n",
    "\n",
    "### Narrative B:\n",
    "{story_b}\n",
    "\n",
    "### Similarity Score:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93b20a1e-b09f-4052-b114-62a088b59161",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf9b43d0-b292-42c9-b769-9236b53c1c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3feb8359-199e-440d-b3c3-e45d0b07a3ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19a7f175-2937-4e65-98f3-45e31ee64338",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def promptify(x1, x2, labels):\n",
    "    user_input = optimized_similarity_user_prompt.format(\n",
    "        story_a=x1, story_b=x2\n",
    "    ).strip()\n",
    "    label = ' '.join([str(score_conversion_func(l)) for l in labels])\n",
    "    model_input = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": optimized_similarity_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_input},\n",
    "            {\"role\": \"assistant\", \"content\": label},\n",
    "        ],\n",
    "        tokenize=False,\n",
    "    )\n",
    "    return {\"text\": model_input}\n",
    "\n",
    "def create_dataset(df, concat_reverse=False, shuffle=False):\n",
    "    score_columns = [f\"score_{i}\" for i in range(len(label_column))]\n",
    "    dataset = (\n",
    "        Dataset.from_pandas(df)\n",
    "        .map(lambda x: promptify(x[\"sentence1\"], x[\"sentence2\"], [x[s] for s in score_columns]))\n",
    "        .remove_columns([\"sentence1\", \"sentence2\"] + score_columns)\n",
    "    )\n",
    "    if concat_reverse:\n",
    "        _ = (\n",
    "            Dataset.from_pandas(df)\n",
    "            .map(lambda x: promptify(x[\"sentence2\"], x[\"sentence1\"], [x[s] for s in score_columns]))\n",
    "            .remove_columns([\"sentence1\", \"sentence2\"] + score_columns)\n",
    "        )\n",
    "        dataset = datasets.concatenate_datasets([dataset, _])\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle()\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33aa4c1f-9b3e-4673-b703-3bfed028fd6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c81f3896301744ad8b508b56758d5454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38c3b2cf20a4013ae247a9fac00e008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754445f9378c4cb5aaaf85b5ede8cd5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb5b1923edb41a99f758dbfb26c5b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = create_dataset(train_df, concat_reverse=True, shuffle=True)\n",
    "dev_dataset = create_dataset(dev_df, concat_reverse=False, shuffle=False)\n",
    "test_dataset = create_dataset(test_df, concat_reverse=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4777d5c-1214-4fa0-b63f-74135e028616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfd357be-94de-479f-a326-92e109e5c847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366e17935b014d0ca31e2a9145096763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained('./sft_llama3-emp_2gpus-summary-3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0498f852-2e93-4957-8fba-556c5cc9b448",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE='cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27fc51ef-2367-4270-8ca5-d9075b62d61b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.to(DEVICE).to(torch.bfloat16).eval()\n",
    "model = model.base_model.merge_and_unload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a48f690-3199-4d23-a843-f9b21757620d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained('meta-llama/Meta-Llama-3-8B-Instruct')\n",
    "tokenizer.pad_token_id = tokenizer.added_tokens_encoder[\n",
    "    \"<|reserved_special_token_0|>\"\n",
    "]\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "generation_config.max_length = 2048\n",
    "\n",
    "generation_config.temperature = 0.0\n",
    "generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "generation_config.do_sample = False\n",
    "model.generation_config.pad_token_id=tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f535f495-f695-4004-87d1-4c96fb5679a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_predict_batch(samples):\n",
    "    model_inputs, labels = zip(*[ sample.split('<|start_header_id|>assistant<|end_header_id|>') for sample in samples['text']])\n",
    "    model_inputs = [ x+'<|start_header_id|>assistant<|end_header_id|>' for x in model_inputs]\n",
    "    # model_input, label = sample['text'].split('<|start_header_id|>assistant<|end_header_id|>')\n",
    "    labels = [ label.strip().split('<')[0] for label in labels]\n",
    "    model_inputs = tokenizer(model_inputs, add_special_tokens=False, return_tensors='pt', padding=True)\n",
    "    \n",
    "    response = model.generate(**{ k:v.to(DEVICE) for k,v in model_inputs.items()}, generation_config=generation_config, pad_token_id=tokenizer.eos_token_id)\n",
    "    return model_inputs, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "845ebeb4-fc8f-4dc4-aace-089cb48509dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minghanwu/.conda/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/minghanwu/.conda/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_inputs, response = test_predict_batch(dev_dataset[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3d3d98e-f2f0-45b0-81db-0dd182ec79a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_batch(samples):\n",
    "    model_inputs, labels = zip(*[ sample.split('<|start_header_id|>assistant<|end_header_id|>') for sample in samples['text']])\n",
    "    # model_input, label = sample['text'].split('<|start_header_id|>assistant<|end_header_id|>')\n",
    "    labels = [ label.strip().split('<')[0] for label in labels]\n",
    "    model_inputs = tokenizer(model_inputs, add_special_tokens=False, return_tensors='pt', padding=True)\n",
    "    \n",
    "    response = model.generate(**{ k:v.to(DEVICE) for k,v in model_inputs.items()}, generation_config=generation_config, pad_token_id=tokenizer.eos_token_id)\n",
    "    predicted_scores = [ tokenizer.decode(response[i][model_inputs.input_ids[i].masked_select(model_inputs.input_ids[i]!=tokenizer.pad_token_id).shape[0]:]).split('\\n')[-1].split('<')[0] for i in range(len(labels))]\n",
    "    predicted_scores = [ multi_scores.split() for multi_scores in predicted_scores]\n",
    "    labels = [ multi_labels.split() for multi_labels in labels]\n",
    "    return predicted_scores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4854a18-04cc-44b9-9f64-2357bd2e0b29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.generation_config.padding_side='left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f771ee77-0396-4e0d-9ee8-9b518cec8375",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41edcdca-611c-4f04-8b27-543a595bd214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_prediction(dataset, bsz=20):\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    for i in tqdm.tqdm(range(len(dataset) // bsz + 1)):\n",
    "        batch = dataset[i*bsz:(i+1)*bsz]\n",
    "        if len(batch['text']) == 0:\n",
    "            break\n",
    "        predicted_score, label = predict_batch(batch)\n",
    "        predictions.extend(predicted_score)\n",
    "        labels.extend(label)\n",
    "    bin_labels = [ [int(float(x)>2.5) for x in multi_labels] for multi_labels in labels]\n",
    "    bin_prediction = [ [int(float(x)>2.5) for x in multi_predictions ] for multi_predictions in predictions]\n",
    "    float_predictions = [ [float(x) for x in multi_predictions ] for multi_predictions in predictions]\n",
    "    float_labels = [ [float(x) for x in multi_labels ] for multi_labels in labels]\n",
    "    return float_predictions, float_labels, bin_prediction, bin_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a789612-0871-40f2-b166-bf2ecb26efd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# float_predictions, float_labels, bin_prediction, bin_labels = make_prediction(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8b5d310-e770-4b9f-befa-8f798401b19c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2f4bd3b-f323-4bf5-8f3a-09e044c6b86b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(dataset):\n",
    "    float_predictions, float_labels, bin_prediction, bin_labels = make_prediction(dataset)\n",
    "    cls_dfs = [ pd.DataFrame(classification_report(bin_label, bin_prediction, output_dict=True)).T for bin_label, bin_prediction in zip(zip(*bin_prediction), zip(*bin_labels))]\n",
    "    cls_metrics = [ precision_recall_fscore_support(bin_label, bin_prediction, average=\"macro\") for bin_label, bin_prediction in zip(zip(*bin_prediction), zip(*bin_labels))]\n",
    "    precision, recall, F1, support = cls_metrics[0]\n",
    "    acc_metrics = [ accuracy_score(bin_label, bin_prediction) for bin_label, bin_prediction in zip(zip(*bin_prediction), zip(*bin_labels))]\n",
    "    accuracy = acc_metrics[0]\n",
    "    spearmanrs = [scipy.stats.spearmanr(np.array(float_label),np.array(float_prediction)).statistic for float_label, float_prediction in zip(zip(*float_predictions), zip(*float_labels))]\n",
    "    pearsonrs = [ scipy.stats.pearsonr(np.array(float_label),np.array(float_prediction)).statistic  for float_label, float_prediction in zip(zip(*float_predictions), zip(*float_labels))]\n",
    "    mses = [  np.square(np.subtract(np.array(float_label),np.array(float_prediction))).mean()  for float_label, float_prediction in zip(zip(*float_predictions), zip(*float_labels))]\n",
    "    sp_df = pd.DataFrame({'S': spearmanrs, 'P': pearsonrs, 'M':mses})\n",
    "    metrics = {\n",
    "        \"accuracy\": round(accuracy, 3),\n",
    "        \"precision\": round(precision, 3),\n",
    "        \"recall\": round(recall, 3),\n",
    "        \"F1\": round(F1, 3),\n",
    "    }\n",
    "    return cls_dfs, sp_df, float_predictions, float_labels, bin_prediction, bin_labels, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8b77217c-1163-4984-8d49-04b220cf26b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/6 [00:00<?, ?it/s]/home/minghanwu/.conda/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/minghanwu/.conda/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      " 83%|█████████████████████████████████████▌       | 5/6 [00:08<00:01,  1.63s/it]\n"
     ]
    }
   ],
   "source": [
    "dev_cls_df, dev_sp_df, predicted_dev_scores, dev_score, dev_bin_predictions, dev_bin_labels, dev_metrics=evaluate(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f10c63-c0e6-41c1-8d02-b08d1dddfc05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18faad9d-6f01-46ed-9ffa-ddb45a08c060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dev_cls_df[0].to_csv('./results/dev_cls.csv',float_format=lambda x:f'{x:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3d594d78-8d5b-4b0e-bb5c-0fe9fc316494",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>0.839695</td>\n",
       "      <td>69.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>31.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.759338</td>\n",
       "      <td>0.785647</td>\n",
       "      <td>0.767673</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.807886</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.795041</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score  support\n",
       "0              0.887097  0.797101  0.839695    69.00\n",
       "1              0.631579  0.774194  0.695652    31.00\n",
       "accuracy       0.790000  0.790000  0.790000     0.79\n",
       "macro avg      0.759338  0.785647  0.767673   100.00\n",
       "weighted avg   0.807886  0.790000  0.795041   100.00"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_cls_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3b473aff-5a2e-4cb0-94b7-cfd5e3fd767f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.79, 'precision': 0.759, 'recall': 0.786, 'F1': 0.768}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d8a23d4-66bd-44ba-8020-7ba4be8005c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      " & precision & recall & f1-score & support \\\\\n",
      "\\midrule\n",
      "0 & 0.887 & 0.797 & 0.840 & 69.000 \\\\\n",
      "1 & 0.632 & 0.774 & 0.696 & 31.000 \\\\\n",
      "accuracy & 0.790 & 0.790 & 0.790 & 0.790 \\\\\n",
      "macro avg & 0.759 & 0.786 & 0.768 & 100.000 \\\\\n",
      "weighted avg & 0.808 & 0.790 & 0.795 & 100.000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dev_cls_df[0].to_latex(float_format=lambda x:f'{x:.3f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "49da6b43-f266-4a86-a495-b5f0f5a30463",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.469965</td>\n",
       "      <td>0.475901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          S         P\n",
       "0  0.469965  0.475901"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_sp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5efdc61-17be-4a3d-8d65-bb10774f3abc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dev_sp_df.rename(columns={'S':'Spearman','P':\"Pearson\"}).to_csv('./results/dev_reg.csv',float_format=lambda x:f'{x:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cf325802-9140-4f25-8831-fe7f6cc4b451",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      " & Spearman & Pearson & MSE \\\\\n",
      "\\midrule\n",
      "0 & 0.470 & 0.476 & 0.497 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dev_sp_df.rename(columns={'S':'Spearman','P':\"Pearson\", \"M\": \"MSE\"}).to_latex(float_format=lambda x:f'{x:.3f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "adbd5744-7a4e-4073-a7c8-f0479aa680b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/21 [00:00<?, ?it/s]/home/minghanwu/.conda/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/minghanwu/.conda/envs/py38/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      " 95%|████████████████████████████████████████▉  | 20/21 [00:33<00:01,  1.66s/it]\n"
     ]
    }
   ],
   "source": [
    "test_cls_df, test_sp_df, predicted_test_scores, test_score, test_bin_predictions, test_bin_labels, test_metrics=evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "82c83a51-7bfe-445a-890b-8e2bd6be64b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.661538</td>\n",
       "      <td>0.578475</td>\n",
       "      <td>0.617225</td>\n",
       "      <td>223.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.541463</td>\n",
       "      <td>0.627119</td>\n",
       "      <td>0.581152</td>\n",
       "      <td>177.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.601501</td>\n",
       "      <td>0.602797</td>\n",
       "      <td>0.599188</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.608405</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.601263</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score  support\n",
       "0              0.661538  0.578475  0.617225    223.0\n",
       "1              0.541463  0.627119  0.581152    177.0\n",
       "accuracy       0.600000  0.600000  0.600000      0.6\n",
       "macro avg      0.601501  0.602797  0.599188    400.0\n",
       "weighted avg   0.608405  0.600000  0.601263    400.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cls_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a5ee57c-1be1-4a27-83c0-a3b1295fd7f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_cls_df[0].to_csv('./results/test_cls.csv',float_format=lambda x:f'{x:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a8af5da-e218-4d31-9e6c-46ecef7c686b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      " & precision & recall & f1-score & support \\\\\n",
      "\\midrule\n",
      "0 & 0.662 & 0.578 & 0.617 & 223.000 \\\\\n",
      "1 & 0.541 & 0.627 & 0.581 & 177.000 \\\\\n",
      "accuracy & 0.600 & 0.600 & 0.600 & 0.600 \\\\\n",
      "macro avg & 0.602 & 0.603 & 0.599 & 400.000 \\\\\n",
      "weighted avg & 0.608 & 0.600 & 0.601 & 400.000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_cls_df[0].to_latex(float_format=lambda x:f'{x:.3f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f9c1bb6-e5e9-49b9-81eb-f0a0dc89449a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spearman</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.30706</td>\n",
       "      <td>0.320428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Spearman   Pearson\n",
       "0   0.30706  0.320428"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sp_df.rename(columns={'S':'Spearman','P':\"Pearson\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "38941a48-64e7-49bc-947f-553220956b6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_sp_df.rename(columns={'S':'Spearman','P':\"Pearson\"}).to_csv('./results/test_reg.csv',float_format=lambda x:f'{x:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "892c9b12-3043-4479-88f1-1823344d9ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      " & Spearman & Pearson & MSE \\\\\n",
      "\\midrule\n",
      "0 & 0.307 & 0.320 & 0.643 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_sp_df.rename(columns={'S':'Spearman','P':\"Pearson\", \"M\": \"MSE\"}).to_latex(float_format=lambda x:f'{x:.3f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318a2b40-4086-484c-8a2d-90bdc0ca22d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad66a11c-bb47-4375-aba4-3cddc2d37184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
