{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e598399e-f2d0-47b1-81e5-586afab41338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vllm\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model = vllm.LLM(\n",
    "    model=model_name,\n",
    "    tokenizer=model_name,\n",
    "    tensor_parallel_size=2,\n",
    "    dtype=\"bfloat16\" if torch.cuda.is_bf16_supported() else \"float16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b07ce9b0-9eb6-4946-9bbd-f4dd193625b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# print(tokenizer.chat_template)\n",
    "# set generation parameters\n",
    "sampling_params = vllm.SamplingParams(max_tokens=512)\n",
    "print(sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e558a6a7-6746-4b44-8fe9-fc59c6222a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "100\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "system_prompt = \"You are a helpful assistant, can conduct great emotional analysis\"\n",
    "user_prompt = \"\"\"Rate the extent to which you agree with the statement: the narrators of the two stories would empathize with each other. We define empathy as feeling, understanding, and relating to what another person is experiencing. Note that it is possible to have empathy even without sharing the exact same experience or circumstance. Importantly, for two stories to be empathetically similar, both narrators should be able to empathize with each other (if narrator A’s story was shared in response to narrator B’s story, narrator B would empathize with narrator A and vice versa). Give your answer on a scale from 1-4 (1-not at all, 2-not so much, 3-very much, 4-extremely)\n",
    "\n",
    "Narrative A: {story_a}\n",
    "\n",
    "Narrative B: {story_b}\n",
    "\n",
    "Rate in a scale of 1-4.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "data_dir = \"./empathic-stories-main/data/\"\n",
    "dataset_name_to_files = {\n",
    "    \"pair-train\": \"PAIRS (train).csv\",\n",
    "    \"pair-dev\": \"PAIRS (dev).csv\",\n",
    "    \"pair-test\": \"PAIRS (test).csv\",\n",
    "    \"story-train\": \"STORIES (train).csv\",\n",
    "    \"story-dev\": \"STORIES (dev).csv\",\n",
    "    \"story-test\": \"STORIES (test).csv\",\n",
    "}\n",
    "\n",
    "for split in [\"train\", \"dev\", \"test\"]:\n",
    "    name = f\"pair-{split}\"\n",
    "    df = pd.read_csv(os.path.join(data_dir, dataset_name_to_files[name]))\n",
    "    print(len(df))\n",
    "# print(df.columns)\n",
    "\n",
    "samples = []\n",
    "for k, v in df.iterrows():\n",
    "    story_a = v['story_A_summary'].strip()\n",
    "    story_b = v['story_B_summary'].strip()\n",
    "    samples.append([story_a, story_b])\n",
    "\n",
    "model_inputs = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\":\"system\", \"content\": system_prompt.strip()}, \n",
    "            {\"role\":\"user\", \"content\": user_prompt.format(story_a=sample[0], story_b=sample[1]).strip()}\n",
    "        ], \n",
    "        tokenize=False)\n",
    "    for sample in samples\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2be3d6de-f88a-4a7e-8c20-aa90b28bd2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant, can conduct great emotional analysis<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Rate the extent to which you agree with the statement: the narrators of the two stories would empathize with each other. We define empathy as feeling, understanding, and relating to what another person is experiencing. Note that it is possible to have empathy even without sharing the exact same experience or circumstance. Importantly, for two stories to be empathetically similar, both narrators should be able to empathize with each other (if narrator A’s story was shared in response to narrator B’s story, narrator B would empathize with narrator A and vice versa). Give your answer on a scale from 1-4 (1-not at all, 2-not so much, 3-very much, 4-extremely)\n",
      "\n",
      "Narrative A: The author has a friend that they value and trust. They recently told their friend about their autism, and their friend reacted positively. The conversation that followed was helpful and enjoyable, and it made a bad day better.\n",
      "\n",
      "Narrative B: The author finished a short book and enjoyed it, which was a contrast to their experience of reading books in school. Their ADHD can make reading comprehension difficult, but they were able to focus. Reading alone can still create a sense of connection and combat feelings of isolation.\n",
      "\n",
      "Rate in a scale of 1-4.\n",
      "\n",
      "Answer:<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(model_inputs[29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c7f22e-2350-4d10-ab77-b0a19a67ae78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████| 20/20 [00:05<00:00,  3.67it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:04<00:00,  4.01it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:06<00:00,  3.22it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:05<00:00,  3.64it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:05<00:00,  3.43it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:05<00:00,  3.64it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:05<00:00,  3.38it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:05<00:00,  3.58it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:06<00:00,  3.23it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:05<00:00,  3.76it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:05<00:00,  3.68it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:06<00:00,  3.01it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:05<00:00,  3.67it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:06<00:00,  2.94it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:05<00:00,  3.50it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:04<00:00,  4.07it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:05<00:00,  3.48it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:05<00:00,  3.85it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:05<00:00,  3.69it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:05<00:00,  3.59it/s]\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "for i in range(20):\n",
    "    batch = model_inputs[i*20: (i+1)*20]\n",
    "    result = model.generate(batch, sampling_params)\n",
    "    r = [ x.outputs[0].text.replace(\"<|start_header_id|>assistant<|end_header_id|>\", \"\").strip() for x in result]\n",
    "    responses += r\n",
    "    pd.DataFrame(responses).to_json(\"pair_test_original_prompt_0501.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bb5017b-1d00-4f9d-b40c-155269c562e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the narratives provided, I would rate the extent to which the narrators of the two stories would empathize with each other as a 3, \"very much\".\\n\\nWhile the narrators may not share the exact same experiences (unwanted attention in Narrative A vs. body shaming and societal pressure in Narrative B), there are common underlying themes and emotions that could lead to empathetic understanding. Both narrators express feelings of discomfort, frustration, and resentment towards external forces that affect their personal lives.\\n\\nNarrative A\\'s experience of unwanted attention and indifference towards consent could resonate with Narrative B\\'s feelings of body shaming and societal pressure. The theme of being objectified, scrutinized, and controlled might be relatable to both narrators. Additionally, both narratives highlight the importance of autonomy, respect, and personal agency, which could facilitate empathetic understanding.\\n\\nWhile they may not share the same specific experiences, the shared emotional and thematic resonance could enable the narrators to empathize with each other\\'s emotions and perspectives.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88dbd00-3779-4d83-86ac-789c6b384d10",
   "metadata": {},
   "source": [
    "### Parse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac38a27b-9e7c-4280-abd9-6a2b5fadb91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_prompt = \"\"\"This is a rater's analysis, please output the score this rater give (1-not at all, 2-not so much, 3-very much, 4-extremely). \n",
    "\n",
    "Analysis: {response}\n",
    "\n",
    "ONLY OUTPUT THE SCORE.\"\"\"\n",
    "\n",
    "model_inputs = [\n",
    "tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\":\"system\", \"content\": system_prompt.strip()}, \n",
    "        {\"role\":\"user\", \"content\": parse_prompt.format(response=response).strip()}\n",
    "    ], \n",
    "    tokenize=False)\n",
    "for response in responses\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "569107cb-a19e-465f-9a73-2d4960e8d634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant, can conduct great emotional analysis<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "This is a rater's analysis, please output the score this rater give (1-not at all, 2-not so much, 3-very much, 4-extremely). \n",
      "\n",
      "Analysis: Based on the two narratives, I would rate the extent to which the narrators would empathize with each other as a 3 - very much.\n",
      "\n",
      "Both narrators have experienced feelings of distress and discomfort as a result of the actions of others. The lesbian woman felt threatened and unsupported by her new neighbors, while the basketball narrator felt hated and belittled by the girl. Both narratives convey a sense of vulnerability and exposure to negative emotions.\n",
      "\n",
      "While the specific circumstances of the two stories differ, the emotional resonance is similar. Both narrators have been on the receiving end of hurtful behavior, and both are seeking validation and understanding. As a result, they can likely identify with each other's emotions and understand the pain and discomfort they are experiencing.\n",
      "\n",
      "The narrators may not share the exact same experiences, but they can empathize with each other's feelings of hurt, frustration, and vulnerability. Furthermore, both stories highlight the importance of community and social support, which emphasizes the potential for mutual understanding and empathy between the two narrators.\n",
      "\n",
      "ONLY OUTPUT THE SCORE.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(model_inputs[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c806d0-56f7-442f-a7d9-73858691c0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "scores = []\n",
    "for i in range(20):\n",
    "    batch = model_inputs[i*20: (i+1)*20]\n",
    "    result = model.generate(batch, sampling_params)\n",
    "    r = [ x.outputs[0].text.replace(\"<|start_header_id|>assistant<|end_header_id|>\", \"\").strip() for x in result]\n",
    "    scores += r\n",
    "    pd.DataFrame(scores).to_json(\"Score_pair_test_original_prompt_0501.json\")\n",
    "\n",
    "scores = [re.findall(r'[1-4]', score) for score in scores]\n",
    "scores = [int(score[0]) for score in scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdc43ab-f783-406b-b70f-28d1d0a8e299",
   "metadata": {},
   "source": [
    "### Evaluate Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc481d24-1297-47b3-9e2e-0aea5e83a154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empathy {'Pearson': 0.231, 'Spearman': 0.22, 'MSE': 0.8}\n",
      "event {'Pearson': 0.25, 'Spearman': 0.245, 'MSE': 1.58}\n",
      "emotion {'Pearson': 0.165, 'Spearman': 0.176, 'MSE': 1.01}\n",
      "moral {'Pearson': 0.193, 'Spearman': 0.203, 'MSE': 1.06}\n"
     ]
    }
   ],
   "source": [
    "from src.eval import eval_sts, eval_nli\n",
    "for h in ['similarity_empathy_human_AGG', 'similarity_event_human_AGG', \n",
    "          'similarity_emotion_human_AGG', 'similarity_moral_human_AGG']:\n",
    "    gold = [int(i) for i in df[h].to_list()]\n",
    "    m1 = eval_sts(gold, scores)\n",
    "    m2 = eval_nli(gold, scores)\n",
    "    print(h.split(\"_\")[1], m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "356ece66-fb39-4995-ac4f-4034f46e9319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empathy {'accuracy': 0.47, 'precision': 0.342, 'recall': 0.288, 'F1': 0.253}\n",
      "event {'accuracy': 0.265, 'precision': 0.422, 'recall': 0.326, 'F1': 0.179}\n",
      "emotion {'accuracy': 0.435, 'precision': 0.35, 'recall': 0.294, 'F1': 0.248}\n",
      "moral {'accuracy': 0.408, 'precision': 0.437, 'recall': 0.293, 'F1': 0.239}\n"
     ]
    }
   ],
   "source": [
    "from src.eval import eval_sts, eval_nli\n",
    "for h in ['similarity_empathy_human_AGG', 'similarity_event_human_AGG', \n",
    "          'similarity_emotion_human_AGG', 'similarity_moral_human_AGG']:\n",
    "    gold = [int(i) for i in df[h].to_list()]\n",
    "    m1 = eval_sts(gold, scores)\n",
    "    m2 = eval_nli(gold, scores)\n",
    "    print(h.split(\"_\")[1], m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293b4765-0e3d-4514-8a76-6e3cfc483601",
   "metadata": {},
   "source": [
    "## LLaMA 3 using its original prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb0f08b-c82b-4c39-841f-8edc76797942",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d2265d6-502f-4503-8528-338002f1d527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "data_dir = \"./empathic-stories-main/data/\"\n",
    "dataset_name_to_files = {\n",
    "    \"pair-train\": \"PAIRS (train).csv\",\n",
    "    \"pair-dev\": \"PAIRS (dev).csv\",\n",
    "    \"pair-test\": \"PAIRS (test).csv\",\n",
    "    \"story-train\": \"STORIES (train).csv\",\n",
    "    \"story-dev\": \"STORIES (dev).csv\",\n",
    "    \"story-test\": \"STORIES (test).csv\",\n",
    "}\n",
    "\n",
    "data = {}\n",
    "for split in [\"train\", \"dev\", \"test\"]:\n",
    "    name = f\"pair-{split}\"\n",
    "    df = pd.read_csv(os.path.join(data_dir, dataset_name_to_files[name]))\n",
    "    data[split] = df\n",
    "    # print(len(df))\n",
    "\n",
    "empathic_similarity_prompt = \"\"\"Rate the extent to which you agree with the statement \"the narrators of the two stories would empathize with each other.\" We define empathy as feeling, understanding, and relating to what another person is experiencing. Note that it is possible to have empathy even without sharing the exact same experience or circumstance. Importantly, for two stories to be empathetically similar, both narrators should be able to empathize with each other (if narrator A’s story was shared in response to narrator B’s story, narrator B would empathize with narrator A and vice versa). Give your answer on a scale from 1-4 (1-not at all, 2-not so much, 3-very much, 4-extremely)\n",
    "\n",
    "Narrative A: {story_a}\n",
    "\n",
    "Narrative B: {story_b}\n",
    "\n",
    "Rate in a scale of 1-4.\n",
    "Answer: \n",
    "\"\"\".strip()\n",
    "\n",
    "brief_empathic_similarity_prompt = \"\"\"Rate how similar two narratives below are, in a scale from 1-4 (1-not at all, 2-not so much, 3-very much, 4-extremely)\n",
    "\n",
    "Narrative A: {story_a}\n",
    "\n",
    "Narrative B: {story_b}\n",
    "\n",
    "Rate in a scale of 1-4.\n",
    "Answer: \"\"\".strip()\n",
    "\n",
    "system_prompt = \"You are a helpful assistant, can conduct great emotional analysis\"\n",
    "\n",
    "split_to_test = \"dev\"\n",
    "df = data[split_to_test]\n",
    "user_inputs = []\n",
    "for k, v in df.iterrows():\n",
    "    story_a = v['story_A_summary'].strip()\n",
    "    story_b = v['story_B_summary'].strip()\n",
    "    user_inputs.append(empathic_similarity_prompt.format(story_a=story_a, story_b=story_b))\n",
    "\n",
    "# print(user_inputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e48d58b-ccc5-451b-970d-4240b1db28fc",
   "metadata": {},
   "source": [
    "### Load model and generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c38d7a8-c867-4c2d-910c-d7ffe3b26e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-05-01 16:57:16,032\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-05-01 16:57:18,581\tINFO worker.py:1749 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-01 16:57:19 llm_engine.py:98] Initializing an LLM engine (v0.4.1) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-01 16:57:21 utils.py:608] Found nccl from library /home/yuxiawang/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=46420)\u001b[0m INFO 05-01 16:57:21 utils.py:608] Found nccl from library /home/yuxiawang/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-01 16:57:21 selector.py:28] Using FlashAttention backend.\n",
      "\u001b[36m(RayWorkerWrapper pid=46420)\u001b[0m INFO 05-01 16:57:21 selector.py:28] Using FlashAttention backend.\n",
      "INFO 05-01 16:57:21 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=46420)\u001b[0m INFO 05-01 16:57:21 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "INFO 05-01 16:57:22 utils.py:129] reading GPU P2P access cache from /home/yuxiawang/.config/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "WARNING 05-01 16:57:22 custom_all_reduce.py:74] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[36m(RayWorkerWrapper pid=46420)\u001b[0m INFO 05-01 16:57:22 utils.py:129] reading GPU P2P access cache from /home/yuxiawang/.config/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[36m(RayWorkerWrapper pid=46420)\u001b[0m WARNING 05-01 16:57:22 custom_all_reduce.py:74] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 05-01 16:57:22 weight_utils.py:193] Using model weights format ['*.safetensors']\n",
      "\u001b[36m(RayWorkerWrapper pid=46420)\u001b[0m INFO 05-01 16:57:23 weight_utils.py:193] Using model weights format ['*.safetensors']\n",
      "INFO 05-01 16:57:24 model_runner.py:173] Loading model weights took 7.4829 GB\n",
      "\u001b[36m(RayWorkerWrapper pid=46420)\u001b[0m INFO 05-01 16:57:25 model_runner.py:173] Loading model weights took 7.4829 GB\n",
      "INFO 05-01 16:57:26 ray_gpu_executor.py:217] # GPU blocks: 12241, # CPU blocks: 4096\n",
      "INFO 05-01 16:57:27 model_runner.py:976] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-01 16:57:27 model_runner.py:980] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=46420)\u001b[0m INFO 05-01 16:57:27 model_runner.py:976] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerWrapper pid=46420)\u001b[0m INFO 05-01 16:57:27 model_runner.py:980] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\n",
      "\u001b[36m(RayWorkerWrapper pid=46420)\u001b[0m [rank1]:[W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerWrapper pid=46420)\u001b[0m INFO 05-01 16:57:30 model_runner.py:1057] Graph capturing finished in 3 secs.\n",
      "INFO 05-01 16:57:30 model_runner.py:1057] Graph capturing finished in 3 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:05<00:00,  3.61it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:06<00:00,  3.25it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:05<00:00,  3.45it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:05<00:00,  3.54it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:05<00:00,  3.79it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.llm import LLaMA3, gpt_easy\n",
    "from src.eval import eval_sts, eval_nli\n",
    "model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model = LLaMA3(model_name=model_name)\n",
    "\n",
    "responses = []\n",
    "batch_size = 20\n",
    "max_output_length = 512\n",
    "save_path = f\"pair_{split_to_test}_original_prompt_0501.json\"\n",
    "for i in range(int(len(user_inputs)/batch_size)):\n",
    "    samples = user_inputs[i*batch_size: (i+1)*batch_size]\n",
    "    r = model.generate(samples, max_output_length=max_output_length, system_prompt = system_prompt)\n",
    "    responses += r\n",
    "    pd.DataFrame(responses).to_json(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f384a78-3dcd-44fa-829d-3d6e4d030e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'd rate the extent to which the narrators of the two stories would empathize with each other a 3 - very much.\\n\\nBoth narratives deal with unexpected events and unexpected kindness. In Narrative A, the group faces a breakdown and unexpected help from a musician and their team, while in Narrative B, the birthday party takes an unexpected turn with the presence of long-lost friends and a heartwarming performance by the children. Both stories feature moments of gratitude, joy, and a sense of community.\\n\\nThe narrators of the two stories could likely understand and relate to each other's experiences, even if they seem vastly different at first glance. They might both appreciate the power of unexpected events and the kindness of strangers (in Narrative A) or the surprise and delight of reuniting with old friends (in Narrative B). They could empathize with the emotional highs and lows, as well as the sense of gratitude and appreciation that comes with these experiences.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad0ce4d-c960-4cdd-8a56-756056524a4f",
   "metadata": {},
   "source": [
    "### Parse and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acd70277-b8c1-4009-b776-6ff829d1264b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████| 20/20 [00:00<00:00, 23.70it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:00<00:00, 22.57it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:00<00:00, 23.22it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:00<00:00, 24.51it/s]\n",
      "Processed prompts: 100%|████████████████████████| 20/20 [00:00<00:00, 23.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 68, 2: 26, 4: 6})\n",
      "\\begin{tabular}{lrrrrrrr}\n",
      "\\toprule\n",
      " & Pearson & Spearman & MSE & accuracy & precision & recall & F1 \\\\\n",
      "\\midrule\n",
      "empathy & 0.291 & 0.305 & 0.680 & 0.490 & 0.281 & 0.319 & 0.275 \\\\\n",
      "event & 0.331 & 0.324 & 1.330 & 0.220 & 0.166 & 0.394 & 0.203 \\\\\n",
      "emotion & 0.249 & 0.250 & 0.910 & 0.330 & 0.237 & 0.312 & 0.230 \\\\\n",
      "moral & 0.218 & 0.216 & 1.000 & 0.320 & 0.213 & 0.304 & 0.200 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "parse_prompt = \"\"\"This is a rater's analysis, please output the score this rater give (1-not at all, 2-not so much, 3-very much, 4-extremely). \n",
    "\n",
    "Analysis: {response}\n",
    "\n",
    "ONLY OUTPUT THE SCORE.\"\"\"\n",
    "user_inputs = [parse_prompt.format(response=response) for response in responses]\n",
    "\n",
    "scores = []\n",
    "for i in range(int(len(user_inputs)/batch_size)):\n",
    "    samples = user_inputs[i*batch_size: (i+1)*batch_size]\n",
    "    r = model.generate(samples, max_output_length=max_output_length, system_prompt = system_prompt)\n",
    "    scores += r\n",
    "    pd.DataFrame(scores).to_json(\"Score_\"+save_path)\n",
    "\n",
    "scores = [re.findall(r'[1-4]', score) for score in scores]\n",
    "scores = [int(score[0]) for score in scores]\n",
    "pd.DataFrame(scores).to_json(\"ParedScore_\"+save_path)\n",
    "print(Counter(scores))\n",
    "\n",
    "results = {}\n",
    "for h in ['similarity_empathy_human_AGG', 'similarity_event_human_AGG', \n",
    "          'similarity_emotion_human_AGG', 'similarity_moral_human_AGG']:\n",
    "    gold_f = [float(i) for i in df[h].to_list()]\n",
    "    gold_c = [int(i) for i in df[h].to_list()]\n",
    "    m = eval_sts(gold_f, scores) \n",
    "    m.update(eval_nli(gold_c, scores))\n",
    "    # m1 = eval_sts(gold_f, scores)\n",
    "    # m2 = eval_nli(gold_c, scores)\n",
    "    # print(h.split(\"_\")[1], m1, m2)\n",
    "    results[h.split(\"_\")[1]] = m\n",
    "print(pd.DataFrame.from_dict(results).T.to_latex(float_format=\"%.3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6778cca9-9177-41ca-b249-bdda8b0b4ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "653c790b-abf7-4543-8473-6df33ddfa546",
   "metadata": {},
   "source": [
    "### Results and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b60679b-8542-41a9-8051-c2a6c000b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from src.eval import eval_sts, eval_nli\n",
    "\n",
    "def bin_label(label):\n",
    "    if label <= 2.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def bin_labels(labels):\n",
    "    return [bin_label(label) for label in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c0f30d-ab9a-4725-bcde-7b8a7b21a527",
   "metadata": {},
   "source": [
    "#### score distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03bbf8e3-7924-402c-9964-d2ebf42a7a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json in the way: scores = pd.read_json(\"Score_pair_dev_original_prompt_0501.json\")\n",
    "# the data is not presented in the order it was saved, the index is not sequentially lines\n",
    "# Next time, remember to check the data index, whether it is 0,1,2,3,4 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a7a4adb-97e6-4cc9-815a-184e8260cfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 325, 2: 55, 4: 17, 1: 3})\n",
      "\\begin{tabular}{lrrrrrrr}\n",
      "\\toprule\n",
      " & Pearson & Spearman & MSE & accuracy & precision & recall & F1 \\\\\n",
      "\\midrule\n",
      "empathy & 0.229 & 0.214 & 0.640 & 0.573 & 0.628 & 0.564 & 0.506 \\\\\n",
      "event & 0.244 & 0.238 & 1.130 & 0.367 & 0.616 & 0.580 & 0.363 \\\\\n",
      "emotion & 0.197 & 0.200 & 0.760 & 0.532 & 0.626 & 0.563 & 0.482 \\\\\n",
      "moral & 0.210 & 0.212 & 0.820 & 0.497 & 0.642 & 0.573 & 0.463 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_to_test = \"test\"\n",
    "scores = pd.read_json(f\"./results/LLaMA3_8B_zs/Score_pair_{split_to_test}_original_prompt_0501.json\", \n",
    "                      typ=\"series\")[0].values()\n",
    "scores = [re.findall(r'[1-4]', score) for score in scores]\n",
    "scores = [int(score[0]) for score in scores]\n",
    "print(Counter(scores))\n",
    "\n",
    "\n",
    "data_dir = \"./empathic-stories-main/data/\"\n",
    "dataset_name = f\"pair-{split_to_test}\"\n",
    "dataset_name_to_files = {\n",
    "    \"pair-train\": \"PAIRS (train).csv\",\n",
    "    \"pair-dev\": \"PAIRS (dev).csv\",\n",
    "    \"pair-test\": \"PAIRS (test).csv\",\n",
    "    \"story-train\": \"STORIES (train).csv\",\n",
    "    \"story-dev\": \"STORIES (dev).csv\",\n",
    "    \"story-test\": \"STORIES (test).csv\",\n",
    "}\n",
    "\n",
    "results = {}\n",
    "# print(os.path.join(data_dir, dataset_name_to_files[dataset_name]))\n",
    "df = pd.read_csv(os.path.join(data_dir, dataset_name_to_files[dataset_name]))\n",
    "for h in ['similarity_empathy_human_AGG', 'similarity_event_human_AGG', \n",
    "          'similarity_emotion_human_AGG', 'similarity_moral_human_AGG']:\n",
    "    gold_f = [float(i) for i in df[h].to_list()]\n",
    "    gold_c = [int(i) if i-int(i)<0.5 else int(i)+1 for i in df[h].to_list()]\n",
    "    m = eval_sts(gold_f, scores) \n",
    "    # m.update(eval_nli(gold_c, scores))\n",
    "    m.update(eval_nli(bin_labels(gold_f), bin_labels(scores)))\n",
    "    results[h.split(\"_\")[1]] = m\n",
    "print(pd.DataFrame.from_dict(results).T.to_latex(float_format=\"%.3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b5e60b4-30a5-48a9-874b-d427c180f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLaMA3 zs summary\n",
    "# dev\n",
    "\\begin{tabular}{lrrrrrrr}\n",
    "\\toprule\n",
    " & Pearson & Spearman & MSE & accuracy & precision & recall & F1 \\\\\n",
    "\\midrule\n",
    "empathy & 0.291 & 0.305 & 0.680 & 0.600 & 0.705 & 0.667 & 0.594 \\\\\n",
    "event & 0.331 & 0.324 & 1.330 & 0.410 & 0.589 & 0.621 & 0.405 \\\\\n",
    "emotion & 0.249 & 0.250 & 0.910 & 0.470 & 0.592 & 0.586 & 0.470 \\\\\n",
    "moral & 0.218 & 0.216 & 1.000 & 0.450 & 0.616 & 0.634 & 0.449 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\n",
    "\n",
    "# test\n",
    "\\begin{tabular}{lrrrrrrr}\n",
    "\\toprule\n",
    " & Pearson & Spearman & MSE & accuracy & precision & recall & F1 \\\\\n",
    "\\midrule\n",
    "empathy & 0.229 & 0.214 & 0.640 & 0.573 & 0.628 & 0.564 & 0.506 \\\\\n",
    "event & 0.244 & 0.238 & 1.130 & 0.367 & 0.616 & 0.580 & 0.363 \\\\\n",
    "emotion & 0.197 & 0.200 & 0.760 & 0.532 & 0.626 & 0.563 & 0.482 \\\\\n",
    "moral & 0.210 & 0.212 & 0.820 & 0.497 & 0.642 & 0.573 & 0.463 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc03f308-e032-4aab-a467-a4d9159b233e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef4c18d-3f94-4ccd-92ce-4398a4037c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35ddb25-624b-410e-b8f5-ea7a97ec4a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe196c01-1a18-44a2-8c39-b725547a6a38",
   "metadata": {},
   "source": [
    "#### No issue with parse score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c8a62bc-1a3b-4056-833d-d4fe6a772fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 68, 2: 26, 4: 6})\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "scores = pd.read_json(\"Score_pair_dev_original_prompt_0501.json\")[0].to_list()\n",
    "scores = [re.findall(r'[1-4]', score) for score in scores]\n",
    "scores = [int(score[0]) for score in scores]\n",
    "print(Counter(scores))\n",
    "\n",
    "purescores = pd.read_json(\"ParedScore_pair_dev_original_prompt_0501.json\")[0].to_list()\n",
    "\n",
    "c = 0\n",
    "for i, j in zip(purescores, scores):\n",
    "    if i != j:\n",
    "        print(\"False\", i, j)\n",
    "        c += 1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bcd16d-4fea-4f21-950f-11e2d094b854",
   "metadata": {},
   "source": [
    "#### Debug Tokenizer issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc945ec-0ff1-491e-a8b2-31d2a8d27f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\" # I used a wrong model name without -Instruct\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model_inputs = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\":\"system\", \"content\": system_prompt.strip()}, \n",
    "            {\"role\":\"user\", \"content\": sample.strip()}\n",
    "        ], \n",
    "        tokenize=False)\n",
    "    for sample in samples\n",
    "]\n",
    "\n",
    "print(model_inputs[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecedb7e-76a1-4d63-bb34-20f4054c351a",
   "metadata": {},
   "source": [
    "#### ChatGPT trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a48a2b7a-d1c5-4319-86ec-376d2f64f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"\"\"Rate the extent to which you agree with the statement \"the narrators of the two stories would empathize with each other.\" We define empathy as feeling, understanding, and relating to what another person is experiencing. Note that it is possible to have empathy even without sharing the exact same experience or circumstance. Importantly, for two stories to be empathetically similar, both narrators should be able to empathize with each other (if narrator A’s story was shared in response to narrator B’s story, narrator B would empathize with narrator A and vice versa). Give your answer on a scale from 1-4 (1-not at all, 2-not so much, 3-very much, 4-extremely)\n",
    "\n",
    "Narrative A: The author experienced unwanted attention from a young age, including being kissed without consent. When she punched the guy who kissed her at 12, she was told she overreacted, causing her to become indifferent to unwanted touching. In college, she learned about consent and discovered that positive experiences are possible with trustworthy people.\n",
    "\n",
    "Narrative B: The author expresses her frustration with societal expectations of female body standards. She resents the pressure to compare her body to others and the trend of certain body types being in style. She also discusses her personal experiences with her family monitoring her body and feeling better treated after losing weight.\n",
    "\n",
    "Rate in a scale of 1-4.\n",
    "Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc113bfe-ad4a-43a4-ab6f-b8b81f7cd97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the narratives provided, I would rate the extent to which the narrators of the two stories would empathize with each other as a 3-very much. Both narrators have shared experiences of feeling uncomfortable and pressured due to societal expectations and unwanted attention in different forms. While their specific experiences may vary, the underlying emotions of frustration, resentment, and the struggle against societal norms are present in both stories, allowing for a strong potential for empathy between the narrators.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_easy(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eecc0e4-4949-431a-90bc-2805ea54d2bb",
   "metadata": {},
   "source": [
    "### 0503 - GPT3.5 Two-stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "360bcc35-53e4-4ef4-af96-3bbe8138ba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = {\n",
    "    \"sys_prompt\": \"You are a helpful assistant, can conduct great emotion and event analysis\", \n",
    "    \"prompt_template\": \"[Story A]\\n{story_a}\\n\\n[Story B]\\n{story_b}\\n\\n[Analysis]\\n{response}\\n[End Analysis]\\n\\n{prompt2}\", \n",
    "    \"defaults\": {\n",
    "        \"prompt1\": \"Compose an analysis comparing two stories from the perspectives of depiction of events, exploration of emotions, the underlying moral lessons and the empathy. Consider how the events unfolded in each story, the emotions evoked by these events, the underlying moral messages conveyed, and the level of empathy the stories elicit from the reader towards the characters involved. Craft your analysis to highlight both the similarities and differences between the stories in each of these four critical aspects.\\n\\n[Story A]\\n{story_a}\\n\\n[Story B]\\n{story_b}\\n\",\n",
    "        \"prompt2\": \"\"\"Rate the similarity of two stories across the dimensions of event portrayal, emotional resonance, moral significance and empathy. Note that rate in a scale of 1 to 4, with 1 indicating minimal alignment and 4 suggesting a high degree of correlation (1-not at all, 2-not so much, 3-very much, 4-extremely). Provide continuous scores based on your comprehensive analysis above and the two stories. \n",
    "        You should only respond in format as described below. DO NOT RETURN ANYTHING ELSE. START YOUR RESPONSE WITH '{{'.\n",
    "        [response format]: \n",
    "{{\n",
    "    \"event\": \"score 1-4 indicating how similar in events\",\n",
    "    \"emotion\": \"score 1-4 indicating how similar in emotional resonance between two stories\",\n",
    "    \"moral\": \"score 1-4 indicating how similar in moral significance\",\n",
    "    \"empathy\": \"score 1-4 indicating the similarity of the level of empathy the stories elicit from the reader towards the characters involved\",\n",
    "}}\"\"\",\n",
    "    }, \n",
    "    \"description\": \"two-stage empathic similarity rating.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f19cedce-3026-4aa9-925e-013920a74b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose an analysis comparing two stories from the perspectives of depiction of events, exploration of emotions, the underlying moral lessons and the empathy. Consider how the events unfolded in each story, the emotions evoked by these events, the underlying moral messages conveyed, and the level of empathy the stories elicit from the reader towards the characters involved. Craft your analysis to highlight both the similarities and differences between the stories in each of these four critical aspects.\n",
      "\n",
      "[Story A]\n",
      "The author and their friends road tripped from Munich to Croatia for a music festival. After driving for five hours, their car experienced a flat tire with no spare and no phone service in the middle of nowhere. They walked four miles to a gas station but found no help, and after waiting for four hours, a musician and their security team picked them up and gave them backstage passes to the festival.\n",
      "\n",
      "[Story B]\n",
      "The author hosted a Hawaiian themed birthday party for their boyfriend in a rented hall. Friends who hadn't been seen since moving to a new city attended and there were unexpected gifts and a dance performed by the couple's young children. The party continued at their house, with some guests spending the night and the next morning filled with laughter and memories.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from argparse import Namespace\n",
    "from src.llm import LLaMA3, gpt_easy\n",
    "from src.eval import eval_sts, eval_nli\n",
    "\n",
    "\n",
    "dataset_name_to_files = {\n",
    "    \"pair-train\": \"PAIRS (train).csv\",\n",
    "    \"pair-dev\": \"PAIRS (dev).csv\",\n",
    "    \"pair-test\": \"PAIRS (test).csv\",\n",
    "    \"story-train\": \"STORIES (train).csv\",\n",
    "    \"story-dev\": \"STORIES (dev).csv\",\n",
    "    \"story-test\": \"STORIES (test).csv\",\n",
    "}\n",
    "\n",
    "split_to_test = \"dev\"\n",
    "data_dir = \"./empathic-stories-main/data/\"\n",
    "model_name=\"gpt-3.5-turbo-0125\"\n",
    "save_path = f\"./results/pair_{split_to_test}_twostage_prompt-{model_name}.json\"\n",
    "    \n",
    "dataset_name = f\"pair-{split_to_test}\"\n",
    "df = pd.read_csv(os.path.join(data_dir, dataset_name_to_files[dataset_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225a672d-e6cc-41b7-9b74-08d26c387aab",
   "metadata": {},
   "source": [
    "#### Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "512b21ce-680f-42c7-9b45-baa5bdbae2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_inputs = []\n",
    "for k, v in df.iterrows():\n",
    "    story_a = v['story_A_summary'].strip()\n",
    "    story_b = v['story_B_summary'].strip()\n",
    "    user_inputs.append(prompt[\"defaults\"][\"prompt1\"].format(story_a=story_a, story_b=story_b))\n",
    "print(user_inputs[0])\n",
    "\n",
    "responses = []\n",
    "for i, user_input in enumerate(user_inputs):\n",
    "    r = gpt_easy(user_input, model=model_name, system_role=prompt[\"sys_prompt\"])\n",
    "    responses.append({\"split\": split_to_test, \"id\": i, \"analysis\": r})\n",
    "    pd.DataFrame(responses).to_json(save_path, lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1a64a4-8b1e-40eb-9e70-c63807ba504a",
   "metadata": {},
   "source": [
    "#### Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbf89b14-fe5d-488f-81d9-d9358c286cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Story A]\n",
      "The author and their friends road tripped from Munich to Croatia for a music festival. After driving for five hours, their car experienced a flat tire with no spare and no phone service in the middle of nowhere. They walked four miles to a gas station but found no help, and after waiting for four hours, a musician and their security team picked them up and gave them backstage passes to the festival.\n",
      "\n",
      "[Story B]\n",
      "The author hosted a Hawaiian themed birthday party for their boyfriend in a rented hall. Friends who hadn't been seen since moving to a new city attended and there were unexpected gifts and a dance performed by the couple's young children. The party continued at their house, with some guests spending the night and the next morning filled with laughter and memories.\n",
      "\n",
      "[Analysis]\n",
      "When comparing Story A and Story B, we can delve into various aspects such as the depiction of events, exploration of emotions, underlying moral lessons, and empathy evoked.\n",
      "\n",
      "In terms of the depiction of events, Story A follows a sequence of events that revolve around a road trip to a music festival and encountering a challenging situation with a car breakdown. On the other hand, Story B portrays a celebratory event - a birthday party - that unfolds with moments of joy, surprise, and connection among friends and family.\n",
      "\n",
      "In both stories, emotions play a significant role. In Story A, the initial emotions of frustration and helplessness due to the car breakdown transition into gratitude and excitement when the characters are helped by strangers. In Story B, emotions of joy, nostalgia, and camaraderie are prevalent throughout the birthday party and the gathering at the house afterward, evoking a sense of warmth and togetherness.\n",
      "\n",
      "Regarding underlying moral lessons, Story A conveys a message of the kindness of strangers and the importance of helping others in need, as demonstrated by the musician and their security team who come to the aid of the stranded group. Story B, on the other hand, emphasizes the value of meaningful connections, celebrating milestones, and creating lasting memories through moments of love and friendship.\n",
      "\n",
      "Empathy towards the characters is evoked in both stories, albeit in different ways. In Story A, readers may empathize with the characters' initial feelings of being stranded and vulnerable, while also feeling a sense of relief and happiness when they are rescued and given a memorable experience at the music festival. In Story B, empathy is elicited through the shared moments of happiness, love, and connection among the characters, allowing readers to feel a sense of warmth and companionship.\n",
      "\n",
      "In conclusion, while Story A and Story B differ in terms of the events depicted and the emotions evoked, both narratives highlight the importance of human connections, acts of kindness, and the richness of shared experiences. Through their unique storytelling approaches, these stories invite readers to empathize with the characters and reflect on the significance of empathy, gratitude, and celebrating life's moments with loved ones.\n",
      "[End Analysis]\n",
      "\n",
      "Rate the similarity of two stories across the dimensions of event portrayal, emotional resonance, moral significance and empathy. Note that rate in a scale of 1 to 4, with 1 indicating minimal alignment and 4 suggesting a high degree of correlation (1-not at all, 2-not so much, 3-very much, 4-extremely). Provide continuous scores based on your comprehensive analysis above and the two stories. \n",
      "        You should only respond in format as described below. DO NOT RETURN ANYTHING ELSE. START YOUR RESPONSE WITH '{{'.\n",
      "        [response format]: \n",
      "{{\n",
      "    \"event\": \"score 1-4 indicating how similar in events\",\n",
      "    \"emotion\": \"score 1-4 indicating how similar in emotional resonance between two stories\",\n",
      "    \"moral\": \"score 1-4 indicating how similar in moral significance\",\n",
      "    \"empathy\": \"score 1-4 indicating the similarity of the level of empathy the stories elicit from the reader towards the characters involved\",\n",
      "}}\n"
     ]
    }
   ],
   "source": [
    "user_inputs = []\n",
    "for k, v in df.iterrows():\n",
    "    story_a = v['story_A_summary'].strip()\n",
    "    story_b = v['story_B_summary'].strip()\n",
    "    user_inputs.append(prompt[\"prompt_template\"].format(story_a=story_a, story_b=story_b, \n",
    "                                                        response=responses[k]['analysis'], \n",
    "                                                        prompt2=prompt[\"defaults\"][\"prompt2\"]))\n",
    "print(user_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0743c99e-970a-437b-80f8-f697f1311c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, user_input in enumerate(user_inputs):\n",
    "    r = gpt_easy(user_input, model=model_name, system_role=prompt[\"sys_prompt\"])\n",
    "    try:\n",
    "        # parse and save into dict\n",
    "        rate = eval(r.strip()[1:-1])\n",
    "    except:\n",
    "        print(i)\n",
    "        rate = r\n",
    "    responses[i].update({\"ratings\": rate})\n",
    "    pd.DataFrame(responses).to_json(\"final_\"+save_path, lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bad1c89-7eb4-4051-a431-66c5cd00925d",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e056ea5c-c33d-4718-a38d-e0a98c7e2c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.read_json(\"final_\"+save_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2a92edaf-57de-4cad-b096-472ac251aef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>id</th>\n",
       "      <th>analysis</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dev</td>\n",
       "      <td>0</td>\n",
       "      <td>When comparing Story A and Story B, we can del...</td>\n",
       "      <td>{'event': 2, 'emotion': 3, 'moral': 3, 'empath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dev</td>\n",
       "      <td>1</td>\n",
       "      <td>In comparing Story A and Story B, several key ...</td>\n",
       "      <td>{'event': 2, 'emotion': 3, 'moral': 3, 'empath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dev</td>\n",
       "      <td>2</td>\n",
       "      <td>In comparing Story A and Story B, we can obser...</td>\n",
       "      <td>{'event': 2, 'emotion': 3, 'moral': 3, 'empath...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  split  id                                           analysis  \\\n",
       "0   dev   0  When comparing Story A and Story B, we can del...   \n",
       "1   dev   1  In comparing Story A and Story B, several key ...   \n",
       "2   dev   2  In comparing Story A and Story B, we can obser...   \n",
       "\n",
       "                                             ratings  \n",
       "0  {'event': 2, 'emotion': 3, 'moral': 3, 'empath...  \n",
       "1  {'event': 2, 'emotion': 3, 'moral': 3, 'empath...  \n",
       "2  {'event': 2, 'emotion': 3, 'moral': 3, 'empath...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "332e696a-1bfe-4c52-8a65-ed689a300da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = []\n",
    "for i, row in scores.iterrows():\n",
    "    r = row[\"ratings\"]\n",
    "    ratings.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e566518a-3508-426e-98cc-4d53bb501239",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1c9c3b8b-29bb-4f0a-8cd8-fc0c89d31039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event</th>\n",
       "      <th>emotion</th>\n",
       "      <th>moral</th>\n",
       "      <th>empathy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   event emotion moral empathy\n",
       "0      2       3     3       3\n",
       "1      2       3     3       3\n",
       "2      2       3     3       3\n",
       "3      2       3     3       3\n",
       "4      2       3     3       4\n",
       "..   ...     ...   ...     ...\n",
       "95     3       3     3       4\n",
       "96     3       3     3       3\n",
       "97     3       2     3       3\n",
       "98     2       3     3       4\n",
       "99     2       3     3       3\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8763ca28-6568-41e9-98a4-ecce590c3bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(data_dir, dataset_name_to_files[dataset_name]))\n",
    "def bin_label(label):\n",
    "    if label <= 2.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def bin_labels(labels):\n",
    "    return [bin_label(label) for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9e53bfce-c413-430a-aa97-eedd707f25d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrr}\n",
      "\\toprule\n",
      " & Pearson & Spearman & MSE & accuracy & precision & recall & F1 \\\\\n",
      "\\midrule\n",
      "empathy-event & 0.314 & 0.327 & 0.610 & 0.720 & 0.716 & 0.667 & 0.673 \\\\\n",
      "event-event & 0.315 & 0.313 & 0.650 & 0.770 & 0.635 & 0.674 & 0.647 \\\\\n",
      "emotion-event & 0.345 & 0.364 & 0.630 & 0.730 & 0.666 & 0.647 & 0.653 \\\\\n",
      "moral-event & 0.286 & 0.288 & 0.610 & 0.750 & 0.636 & 0.649 & 0.642 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for h in ['similarity_empathy_human_AGG', 'similarity_event_human_AGG', \n",
    "            'similarity_emotion_human_AGG', 'similarity_moral_human_AGG']:\n",
    "    gold_f = [float(i) for i in df[h].to_list()]\n",
    "    gold_c = [int(i) if i-int(i)<0.5 else int(i)+1 for i in df[h].to_list()]\n",
    "    \n",
    "    for aspect in ['event', 'emotion', 'moral', 'empathy'][:1]:\n",
    "        scores = [float(i) for i in predictions[aspect].to_list()]\n",
    "    \n",
    "        m = eval_sts(gold_f, scores) \n",
    "        # m.update(eval_nli(gold_c, scores))\n",
    "        m.update(eval_nli(bin_labels(gold_f), bin_labels(scores)))\n",
    "        results[h.split(\"_\")[1]+f\"-{aspect}\"] = m\n",
    "print(pd.DataFrame.from_dict(results).T.to_latex(float_format=\"%.3f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a968359e-5584-4c5e-a0f4-7905e25fb53b",
   "metadata": {},
   "source": [
    "### GPT-4o zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aec1f7-e3ce-4cc0-9c79-5f2a3e5fab4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from argparse import Namespace\n",
    "from src.llm import LLaMA3, gpt_easy\n",
    "from src.eval import eval_sts, eval_nli\n",
    "\n",
    "def bin_label(label):\n",
    "    if label <= 2.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def bin_labels(labels):\n",
    "    return [bin_label(label) for label in labels]\n",
    "\n",
    "optimized_similarity_system_prompt = \"\"\"Rate the extent to which you agree with the statement \"the narrators of the two stories would empathize with each other.\" We define empathy as feeling, understanding, and relating to what another person is experiencing. Note that it is possible to have empathy even without sharing the exact same experience or circumstance. Importantly, for two stories to be empathetically similar, both narrators should be able to empathize with each other (if narrator A’s story was shared in response to narrator B’s story, narrator B would empathize with narrator A and vice versa). Give your answer on a scale from 1-4 (1-not at all, 2-not so much, 3-very much, 4-extremely), with 0.5 increments in each level between 1-4 are allowed. Please only return the score without any explanation.\"\"\".strip()\n",
    "\n",
    "optimized_similarity_user_prompt = \"\"\"\n",
    "### Narrative A:\n",
    "{story_a}\n",
    "\n",
    "### Narrative B:\n",
    "{story_b}\n",
    "\n",
    "### Similarity Score:\n",
    "\"\"\"\n",
    "\n",
    "dataset_name_to_files = {\n",
    "    \"pair-train\": \"PAIRS (train).csv\",\n",
    "    \"pair-dev\": \"PAIRS (dev).csv\",\n",
    "    \"pair-test\": \"PAIRS (test).csv\",\n",
    "    \"story-train\": \"STORIES (train).csv\",\n",
    "    \"story-dev\": \"STORIES (dev).csv\",\n",
    "    \"story-test\": \"STORIES (test).csv\",\n",
    "}\n",
    "\n",
    "def load_es_data(split_to_test, data_dir = \"./empathic-stories-main/data/\"):\n",
    "    dataset_name = f\"pair-{split_to_test}\"\n",
    "    df = pd.read_csv(os.path.join(data_dir, dataset_name_to_files[dataset_name]))\n",
    "    return df\n",
    "\n",
    "def get_save_path(split_to_test, story_type, reorder_story, model_name):\n",
    "    if reorder_story:\n",
    "        suffix = \"reverse\"\n",
    "    else:\n",
    "        suffix = \"\"\n",
    "    save_path = f\"./results/openai_eval/pair_{split_to_test}_{model_name}_{story_type}_zs_{suffix}.json\"\n",
    "    return save_path\n",
    "\n",
    "def gpt_rate(split_to_test, story_type, reorder_story, \n",
    "             model_name = \"gpt-4o\", \n",
    "             data_dir = \"./empathic-stories-main/data/\"):\n",
    "    \n",
    "    df = load_es_data(split_to_test, data_dir)\n",
    "    save_path = get_save_path(split_to_test, story_type, reorder_story, model_name)\n",
    "    print(save_path)\n",
    "    \n",
    "    # construst user prompt\n",
    "    responses = []\n",
    "    for i, row in df.iterrows():\n",
    "        if story_type == \"summary\":\n",
    "            story_a = row['story_A_summary'].strip()\n",
    "            story_b = row['story_B_summary'].strip()   \n",
    "        else:\n",
    "            story_a = row['story_A'].strip()\n",
    "            story_b = row['story_B'].strip()\n",
    "        if reorder_story:\n",
    "            user_input = optimized_similarity_user_prompt.format(story_a=story_b, story_b=story_a)\n",
    "        else:\n",
    "            user_input = optimized_similarity_user_prompt.format(story_a=story_a, story_b=story_b)\n",
    "            # print(user_input)\n",
    "        \n",
    "        # run model to get ratings\n",
    "        r = gpt_easy(user_input, model=model_name, system_role=optimized_similarity_system_prompt)\n",
    "        responses.append({\"split\": split_to_test, \"id\": i, \"score\": r})\n",
    "        pd.DataFrame(responses).to_json(save_path, lines=True, orient=\"records\")\n",
    "    \n",
    "    # eval\n",
    "    responses = pd.DataFrame(responses)\n",
    "    eval_es(responses, df)\n",
    "    return responses\n",
    "\n",
    "def eval_es(responses, df):\n",
    "    # prediction\n",
    "    scores = [float(score) for score in responses_ab[\"score\"].to_list()]\n",
    "    results = {}\n",
    "    for h in ['similarity_empathy_human_AGG', 'similarity_event_human_AGG', \n",
    "                'similarity_emotion_human_AGG', 'similarity_moral_human_AGG']:\n",
    "        gold_f = [float(i) for i in df[h].to_list()]\n",
    "        m = eval_sts(gold_f, scores) \n",
    "        m.update(eval_nli(bin_labels(gold_f), bin_labels(scores)))\n",
    "        results[h.split(\"_\")[1]] = m\n",
    "    print(pd.DataFrame.from_dict(results).T.to_latex(float_format=\"%.3f\"))\n",
    "\n",
    "s\n",
    "def overall_eval(df, save_path):\n",
    "    # overall evaluation, save_path is reverse version savepath\n",
    "    responses_ba = pd.read_json(save_path, lines=True) # story order by b, a\n",
    "    responses_ab = pd.read_json(save_path.replace(\"_reverse\", \"\"), lines=True) # story order by a, b\n",
    "\n",
    "    for score_type in [\"mean\", \"standard\", \"reverse\"]:\n",
    "        print(score_type)\n",
    "        if score_type == \"standard\":\n",
    "            scores = [float(score) for score in responses_ab[\"score\"].to_list()]\n",
    "        elif score_type == \"reverse\":\n",
    "            scores = [float(score) for score in responses_ba[\"score\"].to_list()]\n",
    "        else:\n",
    "            scores = [(float(s1)+float(s2))/2 for s1, s2 in zip(responses_ab[\"score\"].to_list(), responses_ba[\"score\"].to_list())]   \n",
    "        \n",
    "        results = {}\n",
    "        for h in ['similarity_empathy_human_AGG', 'similarity_event_human_AGG', \n",
    "                    'similarity_emotion_human_AGG', 'similarity_moral_human_AGG']:\n",
    "            gold_f = [float(i) for i in df[h].to_list()]\n",
    "            \n",
    "            m = eval_sts(gold_f, scores) \n",
    "            # m.update(eval_nli(gold_c, scores))\n",
    "            m.update(eval_nli(bin_labels(gold_f), bin_labels(scores)))\n",
    "            results[h.split(\"_\")[1]] = m\n",
    "        print(pd.DataFrame.from_dict(results).T.to_latex(float_format=\"%.3f\"))\n",
    "\n",
    "# load data\n",
    "data_dir = \"./empathic-stories-main/data/\"\n",
    "model_name=\"gpt-4o\"\n",
    "# split_to_test = \"dev\"\n",
    "# story_type = \"full\"\n",
    "# reorder_story = True\n",
    "\n",
    "# get ratings\n",
    "for story_type in [\"summary\", \"full\"]:\n",
    "    for split_to_test in ['dev', 'test']:\n",
    "        for reorder_story in [False, True]:\n",
    "            responses = gpt_rate(split_to_test, story_type, reorder_story, \n",
    "                                 model_name = \"gpt-4o\", data_dir = \"./empathic-stories-main/data/\")\n",
    "\n",
    "# run the final overall evaluation\n",
    "for story_type in [\"summary\", \"full\"]:\n",
    "    for split_to_test in ['dev', 'test']:\n",
    "        df = load_es_data(split_to_test, data_dir)\n",
    "        save_path = get_save_path(split_to_test, story_type, reorder_story=True, model_name=model_name)\n",
    "        print(save_path)\n",
    "        overall_eval(df, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b3df8-a444-4f34-a9fa-567736b310ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse story A and B position\n",
    "# GPT-4o zs summary\n",
    "# dev\n",
    "\\begin{tabular}{lrrrrrrr}\n",
    "\\toprule\n",
    " & Pearson & Spearman & MSE & accuracy & precision & recall & F1 \\\\\n",
    "\\midrule\n",
    "empathy & 0.405 & 0.404 & 0.730 & 0.720 & 0.703 & 0.688 & 0.692 \\\\\n",
    "event & 0.408 & 0.415 & 0.780 & 0.730 & 0.628 & 0.697 & 0.635 \\\\\n",
    "emotion & 0.469 & 0.472 & 0.680 & 0.750 & 0.700 & 0.712 & 0.705 \\\\\n",
    "moral & 0.353 & 0.347 & 0.770 & 0.710 & 0.621 & 0.659 & 0.628 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\n",
    "# test\n",
    "\\begin{tabular}{lrrrrrrr}\n",
    "\\toprule\n",
    " & Pearson & Spearman & MSE & accuracy & precision & recall & F1 \\\\\n",
    "\\midrule\n",
    "empathy & 0.368 & 0.376 & 0.720 & 0.650 & 0.658 & 0.652 & 0.648 \\\\\n",
    "event & 0.437 & 0.438 & 0.650 & 0.695 & 0.651 & 0.704 & 0.650 \\\\\n",
    "emotion & 0.399 & 0.398 & 0.680 & 0.645 & 0.642 & 0.638 & 0.638 \\\\\n",
    "moral & 0.422 & 0.438 & 0.640 & 0.700 & 0.687 & 0.689 & 0.688 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\n",
    "\n",
    "# GPT-4o zs full\n",
    "# dev\n",
    "\\begin{tabular}{lrrrrrrr}\n",
    "\\toprule\n",
    " & Pearson & Spearman & MSE & accuracy & precision & recall & F1 \\\\\n",
    "\\midrule\n",
    "empathy & 0.347 & 0.354 & 0.720 & 0.740 & 0.725 & 0.729 & 0.727 \\\\\n",
    "event & 0.335 & 0.336 & 1.000 & 0.690 & 0.629 & 0.720 & 0.620 \\\\\n",
    "emotion & 0.407 & 0.395 & 0.730 & 0.730 & 0.696 & 0.728 & 0.701 \\\\\n",
    "moral & 0.251 & 0.220 & 0.900 & 0.690 & 0.638 & 0.699 & 0.634 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\n",
    "# test\n",
    "\\begin{tabular}{lrrrrrrr}\n",
    "\\toprule\n",
    " & Pearson & Spearman & MSE & accuracy & precision & recall & F1 \\\\\n",
    "\\midrule\n",
    "empathy & 0.298 & 0.307 & 0.760 & 0.610 & 0.611 & 0.611 & 0.610 \\\\\n",
    "event & 0.365 & 0.367 & 0.800 & 0.635 & 0.623 & 0.672 & 0.601 \\\\\n",
    "emotion & 0.319 & 0.327 & 0.760 & 0.615 & 0.613 & 0.614 & 0.613 \\\\\n",
    "moral & 0.354 & 0.367 & 0.720 & 0.665 & 0.661 & 0.667 & 0.660 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512d3c94-737b-4360-8440-6953dd36d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o zs full\n",
    "# dev\n",
    "\\begin{tabular}{lrrrrrrr}\n",
    "\\toprule\n",
    " & Pearson & Spearman & MSE & accuracy & precision & recall & F1 \\\\\n",
    "\\midrule\n",
    "empathy & 0.292 & 0.307 & 0.720 & 0.700 & 0.683 & 0.687 & 0.685 \\\\\n",
    "event & 0.371 & 0.359 & 0.890 & 0.690 & 0.629 & 0.720 & 0.620 \\\\\n",
    "emotion & 0.314 & 0.330 & 0.780 & 0.690 & 0.654 & 0.680 & 0.657 \\\\\n",
    "moral & 0.244 & 0.247 & 0.840 & 0.710 & 0.658 & 0.729 & 0.658 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\n",
    "# test\n",
    "\\begin{tabular}{lrrrrrrr}\n",
    "\\toprule\n",
    " & Pearson & Spearman & MSE & accuracy & precision & recall & F1 \\\\\n",
    "\\midrule\n",
    "empathy & 0.335 & 0.359 & 0.720 & 0.660 & 0.660 & 0.660 & 0.660 \\\\\n",
    "event & 0.385 & 0.404 & 0.810 & 0.615 & 0.627 & 0.678 & 0.590 \\\\\n",
    "emotion & 0.332 & 0.345 & 0.740 & 0.630 & 0.632 & 0.633 & 0.630 \\\\\n",
    "moral & 0.362 & 0.397 & 0.720 & 0.680 & 0.685 & 0.693 & 0.678 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\n",
    "\n",
    "# GPT-4o zs summary\n",
    "# dev\n",
    "\\begin{tabular}{lrrrrrrr}\n",
    "\\toprule\n",
    " & Pearson & Spearman & MSE & accuracy & precision & recall & F1 \\\\\n",
    "\\midrule\n",
    "empathy & 0.486 & 0.486 & 0.700 & 0.750 & 0.739 & 0.717 & 0.723 \\\\\n",
    "event & 0.468 & 0.420 & 0.710 & 0.740 & 0.634 & 0.703 & 0.644 \\\\\n",
    "emotion & 0.489 & 0.477 & 0.690 & 0.780 & 0.734 & 0.743 & 0.738 \\\\\n",
    "moral & 0.429 & 0.416 & 0.710 & 0.740 & 0.652 & 0.696 & 0.662 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\n",
    "# test\n",
    "\\begin{tabular}{lrrrrrrr}\n",
    "\\toprule\n",
    " & Pearson & Spearman & MSE & accuracy & precision & recall & F1 \\\\\n",
    "\\midrule\n",
    "empathy & 0.443 & 0.432 & 0.670 & 0.652 & 0.664 & 0.655 & 0.649 \\\\\n",
    "event & 0.466 & 0.459 & 0.630 & 0.703 & 0.649 & 0.698 & 0.652 \\\\\n",
    "emotion & 0.457 & 0.436 & 0.630 & 0.632 & 0.630 & 0.624 & 0.623 \\\\\n",
    "moral & 0.452 & 0.448 & 0.630 & 0.677 & 0.663 & 0.661 & 0.661 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0640eee2-7f19-4c5d-a9a4-c5d3ffb93bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "python ./src/llm_eval_openllm.py \\\n",
    "    --output_dir results/llm_eval_openllm_small \\\n",
    "    --dataset_dir empathic-stories-main/data \\\n",
    "    --model_name gpt-4o \\\n",
    "    --split dev \\\n",
    "    --prompt_type optimized \\\n",
    "    --story_type summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8316577e-1de7-4b1c-8d3a-7fd29d5dcf11",
   "metadata": {},
   "source": [
    "### OpenAI embedding cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18b5bee8-8b0a-440b-ad6a-e7e7bd0d8f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dataset_name_to_files = {\n",
    "    \"pair-train\": \"PAIRS (train).csv\",\n",
    "    \"pair-dev\": \"PAIRS (dev).csv\",\n",
    "    \"pair-test\": \"PAIRS (test).csv\",\n",
    "    \"story-train\": \"STORIES (train).csv\",\n",
    "    \"story-dev\": \"STORIES (dev).csv\",\n",
    "    \"story-test\": \"STORIES (test).csv\",\n",
    "}\n",
    "\n",
    "split_to_test = \"test\"\n",
    "data_dir = \"./empathic-stories-main/data/\"    \n",
    "dataset_name = f\"pair-{split_to_test}\"\n",
    "df = pd.read_csv(os.path.join(data_dir, dataset_name_to_files[dataset_name]))\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def cal_cosine_sim(a, b):\n",
    "    assert(len(a) == len(b))\n",
    "    # Reshape embeddings to 2D arrays (required by cosine_similarity)\n",
    "    embedding1 = np.array(a).reshape(1, -1)\n",
    "    embedding2 = np.array(b).reshape(1, -1)\n",
    "    \n",
    "    # Step 5: Compute cosine similarity\n",
    "    similarity = cosine_similarity(embedding1, embedding2)\n",
    "    return similarity[0][0]\n",
    "    # print(\"Cosine similarity:\", similarity[0][0])\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"Your_OpenAI_Key\")\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "def get_pair_embedding(story_a, story_b, model=\"text-embedding-3-small\"):\n",
    "    story_a = story_a.replace(\"\\n\", \" \").strip()\n",
    "    story_b = story_b.replace(\"\\n\", \" \").strip()\n",
    "    rs = client.embeddings.create(input = [story_a, story_b], model=model)\n",
    "    a = rs.data[0].embedding\n",
    "    b = rs.data[1].embedding\n",
    "\n",
    "    return a, b, cal_cosine_sim(a, b)\n",
    "\n",
    "embeddings = []\n",
    "model=\"text-embedding-3-large\"\n",
    "is_story_summary = False\n",
    "for i, row in df.iterrows():\n",
    "    if is_story_summary:\n",
    "        story_a = row['story_A_summary']\n",
    "        story_b = row['story_B_summary']\n",
    "        suffix = \"summary\"\n",
    "    else:\n",
    "        story_a = row['story_A']\n",
    "        story_b = row['story_B']\n",
    "        suffix = \"fullstory\"\n",
    "    ae, be, sim = get_pair_embedding(story_a, story_b, model=model)\n",
    "    embeddings.append({\n",
    "        \"story_a_embedding\": ae,\n",
    "        \"story_b_embedding\": be,\n",
    "        \"cosine_sim\": round(sim, 3)\n",
    "    })\n",
    "    pd.DataFrame(embeddings).to_csv(f'./results/{dataset_name}_{model}_embeddings_{suffix}.csv', index=False)\n",
    "emds = pd.DataFrame(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f637051-a66a-4e4f-acc5-75dd34c8006c",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de791d4f-f6d7-4382-8e7c-c8846e7a34c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrr}\n",
      "\\toprule\n",
      " & Pearson & Spearman & MSE & accuracy & precision & recall & F1 \\\\\n",
      "\\midrule\n",
      "empathy & 0.362 & 0.363 & 1.440 & 0.507 & 0.624 & 0.519 & 0.384 \\\\\n",
      "event & 0.488 & 0.469 & 0.590 & 0.782 & 0.737 & 0.551 & 0.538 \\\\\n",
      "emotion & 0.393 & 0.386 & 1.260 & 0.568 & 0.685 & 0.529 & 0.421 \\\\\n",
      "moral & 0.395 & 0.403 & 1.140 & 0.618 & 0.651 & 0.524 & 0.440 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dataset_name_to_files = {\n",
    "    \"pair-train\": \"PAIRS (train).csv\",\n",
    "    \"pair-dev\": \"PAIRS (dev).csv\",\n",
    "    \"pair-test\": \"PAIRS (test).csv\",\n",
    "    \"story-train\": \"STORIES (train).csv\",\n",
    "    \"story-dev\": \"STORIES (dev).csv\",\n",
    "    \"story-test\": \"STORIES (test).csv\",\n",
    "}\n",
    "\n",
    "split_to_test = \"test\"\n",
    "data_dir = \"./empathic-stories-main/data/\"    \n",
    "dataset_name = f\"pair-{split_to_test}\"\n",
    "df = pd.read_csv(os.path.join(data_dir, dataset_name_to_files[dataset_name]))\n",
    "\n",
    "emds = pd.read_csv(f'./results/pair-{split_to_test}_text-embedding-3-large_embeddings_fullstory.csv')\n",
    "preds = emds['cosine_sim'].to_list()\n",
    "\n",
    "from src.eval import eval_sts, eval_nli\n",
    "def bin_label(label):\n",
    "    if label <= 2.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def bin_labels(labels):\n",
    "    return [bin_label(label) for label in labels]\n",
    "\n",
    "results = {}\n",
    "for h in ['similarity_empathy_human_AGG', 'similarity_event_human_AGG', \n",
    "            'similarity_emotion_human_AGG', 'similarity_moral_human_AGG']:\n",
    "    gold_f = [float(i) for i in df[h].to_list()]\n",
    "    gold_c = [int(i) if i-int(i)<0.5 else int(i)+1 for i in df[h].to_list()]\n",
    "    scores = [i*4 for i in preds]\n",
    "    m = eval_sts(gold_f, scores) \n",
    "    # m.update(eval_nli(gold_c, scores))\n",
    "    m.update(eval_nli(bin_labels(gold_f), bin_labels(scores)))\n",
    "    results[h.split(\"_\")[1]] = m\n",
    "print(pd.DataFrame.from_dict(results).T.to_latex(float_format=\"%.3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e59ef5-902c-42ab-b0de-817e1b27fb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full story\n",
    "# dev\n",
    "\\begin{tabular}{lrrrrrrr}\n",
    "\\toprule\n",
    " & Pearson & Spearman & MSE & accuracy & precision & recall & F1 \\\\\n",
    "\\midrule\n",
    "empathy & 0.303 & 0.309 & 1.400 & 0.620 & 0.561 & 0.505 & 0.406 \\\\\n",
    "event & 0.385 & 0.409 & 0.680 & 0.810 & 0.413 & 0.488 & 0.448 \\\\\n",
    "emotion & 0.345 & 0.361 & 1.260 & 0.710 & 0.607 & 0.510 & 0.446 \\\\\n",
    "moral & 0.337 & 0.325 & 1.050 & 0.790 & 0.648 & 0.517 & 0.484 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\n",
    "# test\n",
    "\\begin{tabular}{lrrrrrrr}\n",
    "\\toprule\n",
    " & Pearson & Spearman & MSE & accuracy & precision & recall & F1 \\\\\n",
    "\\midrule\n",
    "empathy & 0.362 & 0.363 & 1.440 & 0.507 & 0.624 & 0.519 & 0.384 \\\\\n",
    "event & 0.488 & 0.469 & 0.590 & 0.782 & 0.737 & 0.551 & 0.538 \\\\\n",
    "emotion & 0.393 & 0.386 & 1.260 & 0.568 & 0.685 & 0.529 & 0.421 \\\\\n",
    "moral & 0.395 & 0.403 & 1.140 & 0.618 & 0.651 & 0.524 & 0.440 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\n",
    "# story summary\n",
    "# dev\n",
    "\\begin{tabular}{lrrrrrrr}\n",
    "\\toprule\n",
    " & Pearson & Spearman & MSE & accuracy & precision & recall & F1 \\\\\n",
    "\\midrule\n",
    "empathy & 0.335 & 0.315 & 1.280 & 0.630 & 0.813 & 0.513 & 0.411 \\\\\n",
    "event & 0.437 & 0.411 & 0.600 & 0.820 & 0.414 & 0.494 & 0.451 \\\\\n",
    "emotion & 0.394 & 0.350 & 1.130 & 0.720 & 0.859 & 0.517 & 0.451 \\\\\n",
    "moral & 0.359 & 0.309 & 0.960 & 0.800 & 0.899 & 0.524 & 0.489 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\n",
    "# test\n",
    "\\begin{tabular}{lrrrrrrr}\n",
    "\\toprule\n",
    " & Pearson & Spearman & MSE & accuracy & precision & recall & F1 \\\\\n",
    "\\midrule\n",
    "empathy & 0.336 & 0.329 & 1.510 & 0.505 & 0.633 & 0.517 & 0.376 \\\\\n",
    "event & 0.485 & 0.465 & 0.620 & 0.780 & 0.738 & 0.542 & 0.522 \\\\\n",
    "emotion & 0.392 & 0.388 & 1.310 & 0.550 & 0.582 & 0.510 & 0.392 \\\\\n",
    "moral & 0.366 & 0.356 & 1.210 & 0.620 & 0.692 & 0.525 & 0.437 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268f89dd-1958-4304-a8e0-f2e7e6bbdb42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302d1d9b-bdd5-4cc2-8d49-84182e36f5c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89745886-f7f4-45bf-b9df-75bbc4b27888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203c135-9d2b-4d56-8e7f-68086f43e28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f28fa9-98fb-4e9a-9df2-e38df7a3ef51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94329145-c740-485d-aff7-4e3ae18f4647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38047dda-bf2c-4cc8-bba4-8dd63be94985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
